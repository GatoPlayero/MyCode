{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20bb376a",
   "metadata": {},
   "source": [
    "### Install delta-spark\n",
    "\n",
    "```\n",
    "pip install delta-spark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f8dd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "#\n",
    "#\n",
    "_builder =\t(\tSparkSession.builder.master('local[1]') \\\n",
    "\t\t\t\t.appName('pyspark-deltalake-local-testing') \\\n",
    "\t\t\t\t.config(\t'spark.sql.extensions'\n",
    "\t\t\t\t\t\t,\t'io.delta.sql.DeltaSparkSessionExtension')\n",
    "\t\t\t\t.config(\t'spark.sql.catalog.spark_catalog'\n",
    "\t\t\t\t\t\t,\t'org.apache.spark.sql.delta.catalog.DeltaCatalog'))\n",
    "#\n",
    "_spark = configure_spark_with_delta_pip(_builder).enableHiveSupport().getOrCreate()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32368ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-deltalake-local-testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x297f7fafd90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark\n",
    "#\n",
    "# try to stop?\n",
    "### _spark.stop()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68fa37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |North America      |\n",
      "|2                |2                         |Northeast           |United States        |North America      |\n",
      "|3                |3                         |Central             |United States        |North America      |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |NA                  |NA                   |NA                 |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_spdf_salesterr = _spark.read \\\n",
    "\t\t\t\t\t.format('csv') \\\n",
    "\t\t\t\t\t.option('inferSchema', True) \\\n",
    "\t\t\t\t\t.option('header',True) \\\n",
    "\t\t\t\t\t.load(\"file:///C:/Users/Administrator/Documents/Code/Python/Spark/DimSalesTerritory.csv\")\n",
    "#\n",
    "_spdf_salesterr.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccc183f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/'\n",
    "#\n",
    "_spdf_salesterr.write \\\n",
    "\t\t\t\t.mode('overwrite') \\\n",
    "\t\t\t\t.format('delta') \\\n",
    "\t\t\t\t.saveAsTable(\t\\\n",
    "\t\t\t\t\t\t\t\t'DimSalesTerritory'\t\\\n",
    "\t\t\t\t\t\t\t,\tpath\t=\t_path\t\\\n",
    "\t\t\t\t\t\t\t,\tformat\t=\t'delta'\t\\\n",
    "\t\t\t\t\t\t\t,\tmode\t=\t'overwrite')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94deb2e5",
   "metadata": {},
   "source": [
    "## DESCRIBE table schema\n",
    "### check the detailed info of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fc5eeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|SalesTerritoryKey           |int                                                                                |NULL   |\n",
      "|SalesTerritoryAlternateKey  |int                                                                                |NULL   |\n",
      "|SalesTerritoryRegion        |string                                                                             |NULL   |\n",
      "|SalesTerritoryCountry       |string                                                                             |NULL   |\n",
      "|SalesTerritoryGroup         |string                                                                             |NULL   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Name                        |spark_catalog.default.dimsalesterritory                                            |       |\n",
      "|Type                        |EXTERNAL                                                                           |       |\n",
      "|Location                    |file:/C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory|       |\n",
      "|Provider                    |delta                                                                              |       |\n",
      "|Table Properties            |[delta.minReaderVersion=1,delta.minWriterVersion=2]                                |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE EXTENDED DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2391f3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|11               |0                         |NA                  |NA                   |NA                 |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|3                |3                         |Central             |United States        |North America      |\n",
      "|2                |2                         |Northeast           |United States        |North America      |\n",
      "|1                |1                         |Northwest           |United States        |North America      |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('SELECT * FROM DimSalesTerritory ORDER BY SalesTerritoryKey DESC;')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf97a5",
   "metadata": {},
   "source": [
    "## DESCRIBE HISTORY\n",
    "\n",
    "### you can check the history of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd68c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                        |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-06-25 11:50:12.308|NULL  |NULL    |CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}|NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 11, numOutputBytes -> 2143}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------------------------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE HISTORY DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adb5bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#from pyspark.dbutils import DBUtils\n",
    "#\n",
    "#_dbutils = DBUtils(_spark)\n",
    "#_dbutils.fs.ls('file:///C:/Users/Administrator/Documents/Code/Python/Spark/SalesTerritory/')\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8809f8",
   "metadata": {},
   "source": [
    "### load transactions into data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2387a135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "|3                |3                         |DELETE              |DELETE               |NULL               |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_spdf_salesterr_trans = _spark.read \\\n",
    "\t\t\t\t\t\t.format('csv') \\\n",
    "\t\t\t\t\t\t.option('inferSchema', True) \\\n",
    "\t\t\t\t\t\t.option('header',True) \\\n",
    "\t\t\t\t\t\t.load(\"file:///C:/Users/Administrator/Documents/Code/Python/Spark/DimSalesTerritoryTransactions1.csv\")\n",
    "#\n",
    "_spdf_salesterr_trans.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9dcfd4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritoryTransactions/'\n",
    "#\n",
    "_spdf_salesterr_trans.write \\\n",
    "\t\t\t\t\t.mode('overwrite') \\\n",
    "\t\t\t\t\t.format('delta') \\\n",
    "\t\t\t\t\t.saveAsTable(\t\\\n",
    "\t\t\t\t\t\t\t\t\t'DimSalesTerritoryTransactions'\t\\\n",
    "\t\t\t\t\t\t\t\t,\tpath\t=\t_path\t\\\n",
    "\t\t\t\t\t\t\t\t,\tformat\t=\t'delta'\t\\\n",
    "\t\t\t\t\t\t\t\t,\tmode\t=\t'overwrite')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0fa2f80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|3                |3                         |DELETE              |DELETE               |NULL               |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('SELECT * FROM DimSalesTerritoryTransactions ORDER BY SalesTerritoryKey ASC;')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef62973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql( \\\n",
    "'''\n",
    "MERGE INTO DimSalesTerritory\t\t\tterr\n",
    "USING DimSalesTerritoryTransactions\t\ttrans\n",
    "ON trans.SalesTerritoryKey = terr.SalesTerritoryKey\n",
    "WHEN MATCHED AND trans.SalesTerritoryRegion = \"DELETE\" THEN\n",
    "\tDELETE\n",
    "WHEN MATCHED THEN\n",
    "\tUPDATE SET SalesTerritoryGroup = trans.SalesTerritoryGroup\n",
    "WHEN NOT MATCHED THEN\n",
    "\tINSERT *\n",
    ";''')\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b799c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                                                                                                                                                                                                                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-06-25 11:50:28.215|NULL  |NULL    |MERGE                            |{predicate -> [\"(SalesTerritoryKey#2570 = SalesTerritoryKey#2565)\"], matchedPredicates -> [{\"predicate\":\"(SalesTerritoryRegion#2572 = DELETE)\",\"actionType\":\"delete\"},{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numTargetRowsCopied -> 8, numTargetRowsDeleted -> 1, numTargetBytesRemoved -> 2143, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, numTargetRowsMatchedDeleted -> 1, numTargetRowsUpdated -> 2, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1394, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2188, executionTimeMs -> 3603, materializeSourceTimeMs -> 1, numTargetRowsInserted -> 1, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2186, numOutputRows -> 11, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|0      |2025-06-25 11:50:12.308|NULL  |NULL    |CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}                                                                                                                                                                                       |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 11, numOutputBytes -> 2143}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE HISTORY DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5f7161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |NA                  |NA                   |NA                 |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_Query_DataFrame = _spark.sql('SELECT * FROM DimSalesTerritory ORDER BY SalesTerritoryKey ASC;')\n",
    "#\n",
    "_resultsAsSQL_Query_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0266e57",
   "metadata": {},
   "source": [
    "## DELTA tables TRANSACTIONS can ONLY and ONLY make changes in a SINGLE TABLE, next statement is <b>INVALID</b> on DeltaLake\n",
    "\n",
    "```SQL\n",
    "BEGIN TRANSACTION;\n",
    "--\n",
    "INSERT INTO Table1 VALUES (1,2);\n",
    "INSERT INTO Table2 VALUES (3,4);\n",
    "--\n",
    "COMMIT;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb59605",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff11696",
   "metadata": {},
   "source": [
    "### Change setting to allow directly query on delta logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f91f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value to replace = False\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "## ## _sc = SparkContext('local[1]', 'pyspark-deltalake-local-testing')\n",
    "## ## _spark.conf.set('spark.databricks.delta.formatCheck.enabled', True)\n",
    "## ## _spark.sparkContext.getConf().getAll()\n",
    "#\n",
    "from pyspark import SparkContext\n",
    "#\n",
    "_sc = _spark.sparkContext\n",
    "_scConf = _sc.getConf()\n",
    "#\n",
    "_scConf.set('spark.databricks.delta.formatCheck.enabled', False)\n",
    "_confValue = _scConf.get('spark.databricks.delta.formatCheck.enabled')\n",
    "#\n",
    "print('value to replace = {0}'.format(_confValue))\n",
    "#\n",
    "## restart spark context\n",
    "#\n",
    "_spark.sparkContext.stop()\n",
    "#\n",
    "## ## ## _spark = SparkSession.builder.config(conf=_scConf).getOrCreate() ### <<<<<<<<<<<<\n",
    "#\n",
    "_builder =\t(\tSparkSession.builder.master('local[1]') \\\n",
    "\t\t\t\t.appName('pyspark-deltalake-local-testing') \\\n",
    "\t\t\t\t.config(conf=_scConf) \\\n",
    "\t\t\t\t.config(\t'spark.sql.extensions'\n",
    "\t\t\t\t\t\t,\t'io.delta.sql.DeltaSparkSessionExtension')\n",
    "\t\t\t\t.config(\t'spark.sql.catalog.spark_catalog'\n",
    "\t\t\t\t\t\t,\t'org.apache.spark.sql.delta.catalog.DeltaCatalog'))\n",
    "#\n",
    "_spark = configure_spark_with_delta_pip(_builder).config(conf=_scConf).getOrCreate()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d942e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new value = False\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_sc = _spark.sparkContext\n",
    "_scConf = _sc.getConf()\n",
    "#\n",
    "_confValue = _scConf.get('spark.databricks.delta.formatCheck.enabled')\n",
    "#\n",
    "print('new value = {0}'.format(_confValue))\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdc4a6",
   "metadata": {},
   "source": [
    "## Let's make a merge using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec688d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |NA                  |NA                   |NA                 |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import pyspark\n",
    "from delta import *\n",
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "#\n",
    "_structSchema = StructType([\t\\\n",
    "\t\t\t\t\t\t\t\tStructField('SalesTerritoryKey',\t\t\tIntegerType(),\tTrue)\t\\\n",
    "\t\t\t\t\t\t\t,\tStructField('SalesTerritoryAlternateKey',\tIntegerType(),\tTrue)\t\\\n",
    "\t\t\t\t\t\t\t,\tStructField('SalesTerritoryRegion',\t\t\tStringType(),\tTrue)\t\\\n",
    "\t\t\t\t\t\t\t,\tStructField('SalesTerritoryCountry',\t\tStringType(),\tTrue)\t\\\n",
    "\t\t\t\t\t\t\t,\tStructField('SalesTerritoryGroup',\t\t\tStringType(),\tTrue)\t\\\n",
    "\t\t\t\t\t\t\t])\n",
    "#\n",
    "_tableForMerge_AsList = [(11,0,'San Juan','Puerto Rico','Caribbean'),(98,-1,'King','United States','Klingon Empire')]\n",
    "_tableForMerge_AsDataFrame = _spark.createDataFrame(_tableForMerge_AsList, _structSchema)\n",
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory'\n",
    "_DimSalesTerritory_AsDeltaTable = DeltaTable.forPath(_spark, _path)\n",
    "_DimSalesTerritory_AsDeltaTable.toDF().show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d98aabdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "## ref : https://medium.com/@ansabiqbal/delta-lake-introduction-with-examples-using-pyspark-cb2a0d7a549d\n",
    "## ref : https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03-delta-lake.html\n",
    "#\n",
    "_DimSalesTerritory_AsDeltaTable\t\\\n",
    "\t.alias('target') \\\n",
    "\t.merge(\t\\\n",
    "\t\t_tableForMerge_AsDataFrame.alias('origin'), \\\n",
    "\t\t\t'target.SalesTerritoryKey = origin.SalesTerritoryKey and target.SalesTerritoryAlternateKey = origin.SalesTerritoryAlternateKey') \\\n",
    "\t.whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29e63d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "|SalesTerritoryKey           |int                                                                                |NULL   |\n",
      "|SalesTerritoryAlternateKey  |int                                                                                |NULL   |\n",
      "|SalesTerritoryRegion        |string                                                                             |NULL   |\n",
      "|SalesTerritoryCountry       |string                                                                             |NULL   |\n",
      "|SalesTerritoryGroup         |string                                                                             |NULL   |\n",
      "|                            |                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                   |       |\n",
      "|Name                        |spark_catalog.default.dimsalesterritory                                            |       |\n",
      "|Type                        |EXTERNAL                                                                           |       |\n",
      "|Location                    |file:/C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory|       |\n",
      "|Provider                    |delta                                                                              |       |\n",
      "|Table Properties            |[delta.minReaderVersion=1,delta.minWriterVersion=2]                                |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/'\n",
    "#\n",
    "_DimSalesTerritory_AsDataFrame = _spark.read.format('delta').load(path=_path)\n",
    "#\n",
    "_DimSalesTerritory_AsDataFrame.write \\\n",
    "\t\t\t\t\t\t\t\t.mode('overwrite') \\\n",
    "\t\t\t\t\t\t\t\t.format('delta') \\\n",
    "\t\t\t\t\t\t\t\t.saveAsTable(\t\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t'DimSalesTerritory'\t\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t,\tpath\t=\t_path\t\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t,\tformat\t=\t'delta'\t\\\n",
    "\t\t\t\t\t\t\t\t\t\t\t,\tmode\t=\t'overwrite')\n",
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE EXTENDED DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "573418d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                                                                                                                                                                                                                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|3      |2025-06-25 11:50:37.898|NULL  |NULL    |CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}                                                                                                                                                                                       |NULL|NULL    |NULL     |2          |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 2230, numOutputRows -> 12, numOutputBytes -> 2230}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|2      |2025-06-25 11:50:35.513|NULL  |NULL    |MERGE                            |{predicate -> [\"((SalesTerritoryKey#4074 = SalesTerritoryKey#4069) AND (SalesTerritoryAlternateKey#4075 = SalesTerritoryAlternateKey#4070))\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []} |NULL|NULL    |NULL     |1          |Serializable  |false        |{numTargetRowsCopied -> 10, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 2188, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 1, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 284, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2230, executionTimeMs -> 1507, materializeSourceTimeMs -> 742, numTargetRowsInserted -> 1, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 477, numOutputRows -> 12, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|1      |2025-06-25 11:50:28.215|NULL  |NULL    |MERGE                            |{predicate -> [\"(SalesTerritoryKey#2570 = SalesTerritoryKey#2565)\"], matchedPredicates -> [{\"predicate\":\"(SalesTerritoryRegion#2572 = DELETE)\",\"actionType\":\"delete\"},{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numTargetRowsCopied -> 8, numTargetRowsDeleted -> 1, numTargetBytesRemoved -> 2143, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, numTargetRowsMatchedDeleted -> 1, numTargetRowsUpdated -> 2, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1394, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2188, executionTimeMs -> 3603, materializeSourceTimeMs -> 1, numTargetRowsInserted -> 1, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2186, numOutputRows -> 11, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1} |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|0      |2025-06-25 11:50:12.308|NULL  |NULL    |CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}                                                                                                                                                                                       |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 11, numOutputBytes -> 2143}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE HISTORY DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b2a74d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |North America      |\n",
      "|2                |2                         |Northeast           |United States        |North America      |\n",
      "|3                |3                         |Central             |United States        |North America      |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |NA                  |NA                   |NA                 |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/'\n",
    "#\n",
    "# Get the version 0 data\n",
    "_original_data = _spark.read.format('delta').option('versionAsOf', 0).load(_path)\n",
    "_original_data.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2daab453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |San Juan            |Puerto Rico          |Caribbean          |\n",
      "|98               |-1                        |King                |United States        |Klingon Empire     |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get the version 0 data\n",
    "_original_data = _spark.read.format('delta').option('versionAsOf', 3).load(_path)\n",
    "_original_data.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57264e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "#\n",
    "def show_log_file(logpath:str, logfilename:str):\n",
    "\tlogFile = os.path.join(logpath,logfilename)\n",
    "\t#\n",
    "\tprint('Processing LogFile : {0}{1}'.format(logFile,'\\n'))\n",
    "\t#\n",
    "\tlogLine = []\n",
    "\ttry:\n",
    "\t\tfor line in open(logFile,'r'):\n",
    "\t\t\ttry:\n",
    "\t\t\t\t_value = json.loads(line)\n",
    "\t\t\t\tlogLine.append(_value)\n",
    "\t\t\texcept json.JSONDecodeError as e:\n",
    "\t\t\t\tprint('Error parsing valid JSON: {0}'.format(line))\n",
    "\texcept Exception as eX:\n",
    "\t\tprint('Errror Reading File : {0}'.format(eX))\n",
    "\t#\n",
    "\tprint('commit Info : {0}\\n'.format(logLine[0]))\n",
    "\tprint('log Block 2 : {0}\\n'.format(logLine[1]))\n",
    "\tprint('log Block 3 : {0}\\n'.format(logLine[2]))\n",
    "\tif len(logLine) > 4:\n",
    "\t\tprint('log Block 4 : {0}\\n'.format(logLine[3]))\n",
    "\t#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da3956c",
   "metadata": {},
   "source": [
    "### Read Delta Log(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e80f9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path = file:/C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory\n",
      "\n",
      "\n",
      "Processing LogFile : C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/_delta_log/00000000000000000000.json\n",
      "\n",
      "commit Info : {'commitInfo': {'timestamp': 1750877412128, 'operation': 'CREATE OR REPLACE TABLE AS SELECT', 'operationParameters': {'partitionBy': '[]', 'clusterBy': '[]', 'description': None, 'isManaged': 'false', 'properties': '{}'}, 'isolationLevel': 'Serializable', 'isBlindAppend': False, 'operationMetrics': {'numFiles': '1', 'numRemovedFiles': '0', 'numRemovedBytes': '0', 'numOutputRows': '11', 'numOutputBytes': '2143'}, 'engineInfo': 'Apache-Spark/4.0.0 Delta-Lake/4.0.0', 'txnId': '13a67fb5-4a25-409f-b18d-419efc1a1631'}}\n",
      "\n",
      "log Block 2 : {'metaData': {'id': 'c78f28d6-7ee6-4386-8960-1871df589752', 'format': {'provider': 'parquet', 'options': {}}, 'schemaString': '{\"type\":\"struct\",\"fields\":[{\"name\":\"SalesTerritoryKey\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SalesTerritoryAlternateKey\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SalesTerritoryRegion\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SalesTerritoryCountry\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"SalesTerritoryGroup\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}', 'partitionColumns': [], 'configuration': {}, 'createdTime': 1750877409578}}\n",
      "\n",
      "log Block 3 : {'protocol': {'minReaderVersion': 1, 'minWriterVersion': 2}}\n",
      "\n",
      "Processing LogFile : C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/_delta_log/00000000000000000001.json\n",
      "\n",
      "commit Info : {'commitInfo': {'timestamp': 1750877428203, 'operation': 'MERGE', 'operationParameters': {'predicate': '[\"(SalesTerritoryKey#2570 = SalesTerritoryKey#2565)\"]', 'matchedPredicates': '[{\"predicate\":\"(SalesTerritoryRegion#2572 = DELETE)\",\"actionType\":\"delete\"},{\"actionType\":\"update\"}]', 'notMatchedPredicates': '[{\"actionType\":\"insert\"}]', 'notMatchedBySourcePredicates': '[]'}, 'readVersion': 0, 'isolationLevel': 'Serializable', 'isBlindAppend': False, 'operationMetrics': {'numTargetRowsCopied': '8', 'numTargetRowsDeleted': '1', 'numTargetBytesRemoved': '2143', 'numTargetDeletionVectorsAdded': '0', 'numTargetRowsMatchedUpdated': '2', 'numTargetRowsMatchedDeleted': '1', 'numTargetRowsUpdated': '2', 'numTargetChangeFilesAdded': '0', 'numTargetRowsNotMatchedBySourceDeleted': '0', 'rewriteTimeMs': '1394', 'numTargetFilesAdded': '1', 'numTargetBytesAdded': '2188', 'executionTimeMs': '3603', 'materializeSourceTimeMs': '1', 'numTargetRowsInserted': '1', 'numTargetDeletionVectorsUpdated': '0', 'scanTimeMs': '2186', 'numOutputRows': '11', 'numTargetDeletionVectorsRemoved': '0', 'numTargetRowsNotMatchedBySourceUpdated': '0', 'numSourceRows': '4', 'numTargetFilesRemoved': '1'}, 'engineInfo': 'Apache-Spark/4.0.0 Delta-Lake/4.0.0', 'txnId': '169cacd2-ab8a-4804-bc70-00dfdaa5db16'}}\n",
      "\n",
      "log Block 2 : {'add': {'path': 'part-00000-017e5992-e0d5-43b1-b191-78101f0dae18-c000.snappy.parquet', 'partitionValues': {}, 'size': 2188, 'modificationTime': 1750877428113, 'dataChange': True, 'stats': '{\"numRecords\":11,\"minValues\":{\"SalesTerritoryKey\":1,\"SalesTerritoryAlternateKey\":0,\"SalesTerritoryRegion\":\"Australia\",\"SalesTerritoryCountry\":\"Australia\",\"SalesTerritoryGroup\":\"Europe\"},\"maxValues\":{\"SalesTerritoryKey\":99,\"SalesTerritoryAlternateKey\":99,\"SalesTerritoryRegion\":\"United Kingdom\",\"SalesTerritoryCountry\":\"United States\",\"SalesTerritoryGroup\":\"Pacific\"},\"nullCount\":{\"SalesTerritoryKey\":0,\"SalesTerritoryAlternateKey\":0,\"SalesTerritoryRegion\":0,\"SalesTerritoryCountry\":0,\"SalesTerritoryGroup\":0}}'}}\n",
      "\n",
      "log Block 3 : {'remove': {'path': 'part-00000-6f56ddbe-fea2-4ae2-8585-e337faac60e9-c000.snappy.parquet', 'deletionTimestamp': 1750877428123, 'dataChange': True, 'extendedFileMetadata': True, 'partitionValues': {}, 'size': 2143, 'stats': '{\"numRecords\":11}'}}\n",
      "\n",
      "Processing LogFile : C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/_delta_log/00000000000000000002.json\n",
      "\n",
      "commit Info : {'commitInfo': {'timestamp': 1750877435510, 'operation': 'MERGE', 'operationParameters': {'predicate': '[\"((SalesTerritoryKey#4074 = SalesTerritoryKey#4069) AND (SalesTerritoryAlternateKey#4075 = SalesTerritoryAlternateKey#4070))\"]', 'matchedPredicates': '[{\"actionType\":\"update\"}]', 'notMatchedPredicates': '[{\"actionType\":\"insert\"}]', 'notMatchedBySourcePredicates': '[]'}, 'readVersion': 1, 'isolationLevel': 'Serializable', 'isBlindAppend': False, 'operationMetrics': {'numTargetRowsCopied': '10', 'numTargetRowsDeleted': '0', 'numTargetBytesRemoved': '2188', 'numTargetDeletionVectorsAdded': '0', 'numTargetRowsMatchedUpdated': '1', 'numTargetRowsMatchedDeleted': '0', 'numTargetRowsUpdated': '1', 'numTargetChangeFilesAdded': '0', 'numTargetRowsNotMatchedBySourceDeleted': '0', 'rewriteTimeMs': '284', 'numTargetFilesAdded': '1', 'numTargetBytesAdded': '2230', 'executionTimeMs': '1507', 'materializeSourceTimeMs': '742', 'numTargetRowsInserted': '1', 'numTargetDeletionVectorsUpdated': '0', 'scanTimeMs': '477', 'numOutputRows': '12', 'numTargetDeletionVectorsRemoved': '0', 'numTargetRowsNotMatchedBySourceUpdated': '0', 'numSourceRows': '2', 'numTargetFilesRemoved': '1'}, 'engineInfo': 'Apache-Spark/4.0.0 Delta-Lake/4.0.0', 'txnId': 'd3b18900-dd68-47c9-880d-38b0c57206a6'}}\n",
      "\n",
      "log Block 2 : {'add': {'path': 'part-00000-c48a347c-c564-46ae-b0f8-34281d1ef97b-c000.snappy.parquet', 'partitionValues': {}, 'size': 2230, 'modificationTime': 1750877435503, 'dataChange': True, 'stats': '{\"numRecords\":12,\"minValues\":{\"SalesTerritoryKey\":1,\"SalesTerritoryAlternateKey\":-1,\"SalesTerritoryRegion\":\"Australia\",\"SalesTerritoryCountry\":\"Australia\",\"SalesTerritoryGroup\":\"Caribbean\"},\"maxValues\":{\"SalesTerritoryKey\":99,\"SalesTerritoryAlternateKey\":99,\"SalesTerritoryRegion\":\"United Kingdom\",\"SalesTerritoryCountry\":\"United States\",\"SalesTerritoryGroup\":\"Pacific\"},\"nullCount\":{\"SalesTerritoryKey\":0,\"SalesTerritoryAlternateKey\":0,\"SalesTerritoryRegion\":0,\"SalesTerritoryCountry\":0,\"SalesTerritoryGroup\":0}}'}}\n",
      "\n",
      "log Block 3 : {'remove': {'path': 'part-00000-017e5992-e0d5-43b1-b191-78101f0dae18-c000.snappy.parquet', 'deletionTimestamp': 1750877435503, 'dataChange': True, 'extendedFileMetadata': True, 'partitionValues': {}, 'size': 2188, 'stats': '{\"numRecords\":11}'}}\n",
      "\n",
      "Processing LogFile : C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/_delta_log/00000000000000000003.json\n",
      "\n",
      "commit Info : {'commitInfo': {'timestamp': 1750877437898, 'operation': 'CREATE OR REPLACE TABLE AS SELECT', 'operationParameters': {'partitionBy': '[]', 'clusterBy': '[]', 'description': None, 'isManaged': 'false', 'properties': '{}'}, 'readVersion': 2, 'isolationLevel': 'Serializable', 'isBlindAppend': False, 'operationMetrics': {'numFiles': '1', 'numRemovedFiles': '1', 'numRemovedBytes': '2230', 'numOutputRows': '12', 'numOutputBytes': '2230'}, 'engineInfo': 'Apache-Spark/4.0.0 Delta-Lake/4.0.0', 'txnId': '9a7f7da9-43dd-4b09-901e-8bb84fcb23aa'}}\n",
      "\n",
      "log Block 2 : {'add': {'path': 'part-00000-28c59187-da3f-4d8c-991b-e78a998d7167-c000.snappy.parquet', 'partitionValues': {}, 'size': 2230, 'modificationTime': 1750877437677, 'dataChange': True, 'stats': '{\"numRecords\":12,\"minValues\":{\"SalesTerritoryKey\":1,\"SalesTerritoryAlternateKey\":-1,\"SalesTerritoryRegion\":\"Australia\",\"SalesTerritoryCountry\":\"Australia\",\"SalesTerritoryGroup\":\"Caribbean\"},\"maxValues\":{\"SalesTerritoryKey\":99,\"SalesTerritoryAlternateKey\":99,\"SalesTerritoryRegion\":\"United Kingdom\",\"SalesTerritoryCountry\":\"United States\",\"SalesTerritoryGroup\":\"Pacific\"},\"nullCount\":{\"SalesTerritoryKey\":0,\"SalesTerritoryAlternateKey\":0,\"SalesTerritoryRegion\":0,\"SalesTerritoryCountry\":0,\"SalesTerritoryGroup\":0}}'}}\n",
      "\n",
      "log Block 3 : {'remove': {'path': 'part-00000-c48a347c-c564-46ae-b0f8-34281d1ef97b-c000.snappy.parquet', 'deletionTimestamp': 1750877437898, 'dataChange': True, 'extendedFileMetadata': True, 'partitionValues': {}, 'size': 2230}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE EXTENDED DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame = _resultsAsSQL_DataFrame.where(col('col_name') == 'Location').select(col('data_type').alias('value'))\n",
    "#\n",
    "_resultsAsSQL_AsPandas = _resultsAsSQL_DataFrame.toPandas()\n",
    "_resultsAsSQL_AsDict = _resultsAsSQL_AsPandas.to_dict()\n",
    "#\n",
    "print('Path = {0}\\n\\n'.format(_resultsAsSQL_AsDict['value'][0]))\n",
    "#\n",
    "_fixedPath = str('{0}/_delta_log'.format((_resultsAsSQL_AsDict['value'][0]).replace('file:/','')))\n",
    "_filesonPath = os.listdir(_fixedPath)\n",
    "_json_files = [f for f in _filesonPath if f.endswith('.json')]\n",
    "#\n",
    "for _cF in _json_files:\n",
    "\tshow_log_file('{0}/'.format(_fixedPath), _cF)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4c9ba",
   "metadata": {},
   "source": [
    "### DESCRIBE HISTORY using <b>%fs</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bac30077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation                        |operationParameters                                                                                                                                                                                                                                                                   |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-06-25 11:50:12.308|NULL  |NULL    |CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}                                                                                                                                                                                       |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 11, numOutputBytes -> 2143}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|1      |2025-06-25 11:50:28.215|NULL  |NULL    |MERGE                            |{predicate -> [\"(SalesTerritoryKey#2570 = SalesTerritoryKey#2565)\"], matchedPredicates -> [{\"predicate\":\"(SalesTerritoryRegion#2572 = DELETE)\",\"actionType\":\"delete\"},{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numTargetRowsCopied -> 8, numTargetRowsDeleted -> 1, numTargetBytesRemoved -> 2143, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 2, numTargetRowsMatchedDeleted -> 1, numTargetRowsUpdated -> 2, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 1394, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2188, executionTimeMs -> 3603, materializeSourceTimeMs -> 1, numTargetRowsInserted -> 1, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 2186, numOutputRows -> 11, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 4, numTargetFilesRemoved -> 1} |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|2      |2025-06-25 11:50:35.513|NULL  |NULL    |MERGE                            |{predicate -> [\"((SalesTerritoryKey#4074 = SalesTerritoryKey#4069) AND (SalesTerritoryAlternateKey#4075 = SalesTerritoryAlternateKey#4070))\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []} |NULL|NULL    |NULL     |1          |Serializable  |false        |{numTargetRowsCopied -> 10, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 2188, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 1, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 284, numTargetFilesAdded -> 1, numTargetBytesAdded -> 2230, executionTimeMs -> 1507, materializeSourceTimeMs -> 742, numTargetRowsInserted -> 1, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 477, numOutputRows -> 12, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|3      |2025-06-25 11:50:37.898|NULL  |NULL    |CREATE OR REPLACE TABLE AS SELECT|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}                                                                                                                                                                                       |NULL|NULL    |NULL     |2          |Serializable  |false        |{numFiles -> 1, numRemovedFiles -> 1, numRemovedBytes -> 2230, numOutputRows -> 12, numOutputBytes -> 2230}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_fixedPath = str('{0}'.format((_resultsAsSQL_AsDict['value'][0]).replace('file:/','')))\n",
    "_query = \"DESCRIBE HISTORY '{0}'\".format(_fixedPath)\n",
    "_resultsAsSQL_DataFrame = _spark.sql(_query)\n",
    "#\n",
    "_resultsAsSQL_DataFrame.sort(col('version').asc()).show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b65e3",
   "metadata": {},
   "source": [
    "### SELECT from delta path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b9ac15d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY : SELECT * FROM delta.`file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/`\n",
      "\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |San Juan            |Puerto Rico          |Caribbean          |\n",
      "|98               |-1                        |King                |United States        |Klingon Empire     |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## path could be executed as SELECT statement too..\n",
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/'\n",
    "_query = \"SELECT * FROM delta.`{0}`\".format(_path)\n",
    "#\n",
    "print('QUERY : {0}\\n'.format(_query))\n",
    "_resultsAsSQL_DataFrame = _spark.sql(_query)\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b614d77",
   "metadata": {},
   "source": [
    "### CREATE table FROM path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5dd13b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY : CREATE TABLE IF NOT EXISTS DimSalesTerritory USING delta LOCATION 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/'\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|SalesTerritoryKey|SalesTerritoryAlternateKey|SalesTerritoryRegion|SalesTerritoryCountry|SalesTerritoryGroup|\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "|1                |1                         |Northwest           |United States        |Federation         |\n",
      "|2                |2                         |Northeast           |United States        |Klingon Empire     |\n",
      "|4                |4                         |Southwest           |United States        |North America      |\n",
      "|5                |5                         |Southeast           |United States        |North America      |\n",
      "|6                |6                         |Canada              |Canada               |North America      |\n",
      "|7                |7                         |France              |France               |Europe             |\n",
      "|8                |8                         |Germany             |Germany              |Europe             |\n",
      "|9                |9                         |Australia           |Australia            |Pacific            |\n",
      "|10               |10                        |United Kingdom      |United Kingdom       |Europe             |\n",
      "|11               |0                         |San Juan            |Puerto Rico          |Caribbean          |\n",
      "|98               |-1                        |King                |United States        |Klingon Empire     |\n",
      "|99               |99                        |Kalos               |Cronos               |Klingon Empire     |\n",
      "+-----------------+--------------------------+--------------------+---------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/Spark/MyDeltaLake/SalesTerritory/'\n",
    "_query = \"CREATE TABLE IF NOT EXISTS DimSalesTerritory USING delta LOCATION '{0}'\".format(_path)\n",
    "#\n",
    "print('QUERY : {0}'.format(_query))\n",
    "_resultsAsSQL_DataFrame = _spark.sql(_query)\n",
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('SELECT * FROM DimSalesTerritory')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(50, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8dd517",
   "metadata": {},
   "source": [
    "## Databricks Delta Lake Tables: Managed vs Unmanaged\n",
    "\n",
    "### Managed vs Unmanaged Delta Lake Table\n",
    "\n",
    "Delta Lake is a powerful storage layer for big data processing workloads in Databricks. In the previous article, we discussed [Delta Lake on Databricks: Python Installation and Setup Guide](/delta-lake-on-databricks-python-installation-and-setup-guide-25c9a9bd11ed)  \n",
    "When working with Delta Lake tables, you can choose between two types of tables: managed and unmanaged. In this article, well explore the key differences between these two types of tables, and how theyre used in Databricks.\n",
    "\n",
    "Managed Delta Table\n",
    "-------------------\n",
    "\n",
    "Managed Delta Tables are tables whose metadata and data are managed by Delta Lake. You can create a managed Delta table using the SQL API or Python API in Databricks. Managed tables manage the storage and location of data and the table schema. Heres an example of how to create a managed Delta table using the SQL API:\n",
    "\n",
    "```\n",
    "CREATE TABLE my_table (\n",
    "  id INT,\n",
    "  name STRING\n",
    ")\n",
    "USING DELTA;\n",
    "```\n",
    "\n",
    "\n",
    "Unmanaged Delta Table\n",
    "---------------------\n",
    "\n",
    "On the other hand, Unmanaged Delta Tables are tables whose metadata is managed by Delta Lake, but data is managed externally. You can create unmanaged Delta tables using the SQL API or Python API in Databricks. Heres an example of how to create an unmanaged Delta table using the SQL API:\n",
    "\n",
    "```\n",
    "CREATE TABLE my_table\n",
    "USING DELTA\n",
    "LOCATION '/mnt/delta/my_table';\n",
    "```\n",
    "\n",
    "\n",
    "Managed vs Unmanaged Tables\n",
    "---------------------------\n",
    "\n",
    "There are several differences between managed and unmanaged tables in Delta Lake. Here are a few key differences:\n",
    "\n",
    "*   **Storage Location**: Managed tables are stored in a location managed by Delta Lake, while unmanaged tables are stored in an external location managed by the user.\n",
    "*   **Data Management**: Managed tables manage both metadata and data, while unmanaged tables manage only metadata.\n",
    "*   **Schema Management**: Both managed and unmanaged tables manage the schema of the table.\n",
    "*   **Performance**: Managed tables are generally faster than unmanaged tables because they have better control over the storage and access of the data.\n",
    "*   **Dropping Table**: When you drop a managed Delta table, both the table metadata and data are deleted from the storage layer. However, when you drop an unmanaged Delta table, only the table metadata is deleted, and the data remains intact in the external storage layer. Therefore, you need to be careful when dropping unmanaged tables, as you could lose your data if youre not careful.\n",
    "\n",
    "In conclusion, Delta Lake tables in Databricks can be managed or unmanaged, and understanding the differences between the two is crucial to optimizing your big data processing workflows. Whether you need the flexibility of unmanaged tables or the power of managed tables, Delta Lake has you covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca03f55c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
