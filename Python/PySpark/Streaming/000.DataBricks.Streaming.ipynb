{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27f7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### https://www.youtube.com/watch?v=r7FTCuTl84g&t=02h22m20s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a8346",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55279ba",
   "metadata": {},
   "source": [
    "# 🔄 What is Shuffle in Apache Spark?\n",
    "\n",
    "In Apache Spark, **shuffle** refers to the process of **redistributing data** across partitions. It happens when Spark needs to **group, join,** or **aggregate** data that is spread across different nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Why Does Shuffle Happen?\n",
    "\n",
    "Shuffle occurs when operations require data from **multiple partitions** to be **reorganized**. This is common in:\n",
    "\n",
    "- `groupByKey()`\n",
    "- `reduceByKey()`\n",
    "- `join()`\n",
    "- `distinct()`\n",
    "- `repartition()`\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 How Shuffle Works\n",
    "\n",
    "1. **Map Phase**: Spark processes data and prepares it for shuffling.\n",
    "2. **Shuffle Phase**: Data is moved across the network to the correct partitions.\n",
    "3. **Reduce Phase**: Spark performs the final computation on the shuffled data.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚠️ Impacts of Shuffling\n",
    "\n",
    "| Impact            | Description |\n",
    "|-------------------|-------------|\n",
    "| ⏱ Performance     | Shuffling is expensive and can slow down jobs. |\n",
    "| 🔌 Network I/O     | Data is transferred between nodes, increasing network traffic. |\n",
    "| 💾 Disk Usage      | Intermediate data may be written to disk. |\n",
    "| 🧠 Memory Pressure | Can cause out-of-memory errors if not managed well. |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Diagram: Shuffle in Action\n",
    "\n",
    "```plaintext\n",
    "Before Shuffle:             After Shuffle:\n",
    "+---------+                +---------+\n",
    "| Part 1  |----\\     /---->| Part A  |\n",
    "| (A, 1)  |     \\   /      | (A, 1)  |\n",
    "| (B, 2)  |      \\ /       | (B, 2)  |\n",
    "+---------+       X        +---------+\n",
    "| Part 2  |      / \\       | Part B  |\n",
    "| (A, 3)  |     /   \\      | (A, 3)  |\n",
    "| (C, 4)  |----/     \\---->| (C, 4)  |\n",
    "+---------+                +---------+\n",
    "```\n",
    "\n",
    "## ✅ How to Minimize Shuffle\n",
    "\n",
    "|\tStrategy\t\t\t|\tBenefit\t\t\t\t\t|\n",
    "|-----------------------|---------------------------|\n",
    "|\tUse `reduceByKey()`\t|\tCombines data before shuffling\t|\n",
    "|\tUse `broadcast()`\t\t|\tAvoids shuffling large datasets during joins\t|\n",
    "|\tUse `partitionBy()`\t|\tControls data distribution\t|\n",
    "|\tAvoid `groupByKey()`\t|\tUse `reduceByKey()` or `aggregateByKey()` instead\t|\n",
    "\n",
    "### 💡 Code Example (Scala)\n",
    "\n",
    "```scala\n",
    "val data = sc.parallelize(Seq((\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4)), 2)\n",
    "// This causes a shuffle\n",
    "val result = data.reduceByKey(_ + _)\n",
    "result.collect()\n",
    "// Output: Array((\"A\", 4), (\"B\", 2), (\"C\", 4))\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 🚀 Deeper Dive: Broadcast Joins & Partitioning Strategies in Spark\n",
    "\n",
    "---\n",
    "\n",
    "## 📡 Broadcast Joins\n",
    "\n",
    "A **broadcast join** is a special type of join in Spark where a **small dataset** is sent to **all worker nodes**. This avoids shuffling the large dataset across the network.\n",
    "\n",
    "### ✅ When to Use\n",
    "\n",
    "- One dataset is **very small** (can fit in memory)\n",
    "- Joining with a **large dataset**\n",
    "- Want to **avoid shuffle** for faster performance\n",
    "\n",
    "### ⚡ Benefits\n",
    "\n",
    "| Benefit           | Description |\n",
    "|-------------------|-------------|\n",
    "| 🚫 No shuffle     | Small data is copied, not moved |\n",
    "| ⚡ Faster joins    | Reduces network traffic |\n",
    "| 💾 Saves memory    | Efficient for small lookup tables |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧭 Partitioning Strategies\n",
    "\n",
    "**Partitioning** controls how data is split across Spark's workers. Good partitioning helps Spark process data **efficiently** and **avoid unnecessary shuffles**.\n",
    "\n",
    "### 🔧 Types of Partitioning\n",
    "\n",
    "| Strategy         | Description |\n",
    "|------------------|-------------|\n",
    "| `HashPartitioner` | Uses hash of key to assign partitions |\n",
    "| `RangePartitioner`| Splits data based on key ranges |\n",
    "| `CustomPartitioner`| User-defined logic for partitioning |\n",
    "\n",
    "### ✅ Why It Matters\n",
    "\n",
    "- Ensures **related data is together**\n",
    "- Reduces **shuffle during joins or aggregations**\n",
    "- Improves **parallelism and performance**\n",
    "\n",
    "---\n",
    "\n",
    "## 🆚 Broadcast Join vs Partitioning\n",
    "\n",
    "| Feature             | Broadcast Join         | Partitioning Strategy     |\n",
    "|---------------------|------------------------|----------------------------|\n",
    "| Use case            | Small + large dataset  | Large datasets             |\n",
    "| Shuffle avoided     | ✅ Yes                 | ✅ If partitioned well     |\n",
    "| Memory usage        | 🧠 Small dataset only   | 💾 Depends on partition size |\n",
    "| Setup complexity    | 🟢 Simple               | 🔴 May require tuning       |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 Code Examples (Scala)\n",
    "\n",
    "```scala\n",
    "// Broadcast Join Example\n",
    "val small = sc.parallelize(Seq((1, \"A\"), (2, \"B\")))\n",
    "val large = sc.parallelize(Seq((1, 100), (2, 200), (3, 300)))\n",
    "\n",
    "val broadcastSmall = sc.broadcast(small.collectAsMap())\n",
    "val joined = large.map { case (id, value) =>\n",
    "  (id, value, broadcastSmall.value.getOrElse(id, \"Unknown\"))\n",
    "}\n",
    "```\n",
    "\n",
    "```scala\n",
    "// Partitioning Example\n",
    "val data = sc.parallelize(Seq((\"A\", 1), (\"B\", 2), (\"A\", 3), (\"C\", 4)))\n",
    "val partitioned = data.partitionBy(new org.apache.spark.HashPartitioner(2))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134522be",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48275693",
   "metadata": {},
   "source": [
    "# Understanding Narrow and Wide Transformations in Apache Spark \n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "In Apache Spark, transformations form the backbone of how data is processed across distributed clusters. A transformation in Spark is an operation that produces a new RDD (*Resilient Distributed Dataset*) or DataFrame from an existing one, setting the stage for distributed data processing. Broadly, these transformations fall into two categories — **narrow transformations** and **wide transformations** — each impacting data flow, efficiency, and the overall performance of Spark applications.\n",
    "\n",
    "In this article, we’ll delve into both types of transformations, explain their distinctions, and demonstrate each with code examples in RDDs and DataFrames.\n",
    "\n",
    "**1\\. Narrow Transformations**\n",
    "------------------------------\n",
    "\n",
    "A **narrow transformation** is one where each output partition depends solely on a single input partition. This means that the data needed to generate an output partition resides within the same partition as the input data. As a result, Spark can process these transformations without moving data between nodes, minimizing network overhead and improving processing speed.\n",
    "\n",
    "**Key Characteristics of Narrow Transformations:**\n",
    "--------------------------------------------------\n",
    "\n",
    "*   No data shuffle between nodes.\n",
    "*   Localized computation within partitions.\n",
    "*   Faster execution due to minimized network I/O.\n",
    "\n",
    "## **Examples of Narrow Transformations**\n",
    "\n",
    "#### **map()**\n",
    "---------\n",
    "\n",
    "The map() function applies a given function to each element in an RDD or DataFrame, without requiring any data exchange between partitions.\n",
    "\n",
    "**RDD Example**\n",
    "\n",
    "```python\n",
    "# Assuming SparkContext (sc) is already initialized\n",
    "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "mapped_rdd = rdd.map(lambda x: x * 2)\n",
    "print(mapped_rdd.collect())  # Output: [2, 4, 6, 8, 10]\n",
    "```\n",
    "\n",
    "**DataFrame Example**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"NarrowTransformationExample\").getOrCreate()\n",
    "df = spark.createDataFrame([(1,), (2,), (3,), (4,), (5,)], [\"value\"])\n",
    "mapped_df = df.withColumn(\"doubled_value\", col(\"value\") * 2)\n",
    "mapped_df.show()  # Output shows original and doubled values\n",
    "```\n",
    "\n",
    "#### **filter()**\n",
    "------------\n",
    "\n",
    "The filter() transformation retains elements that satisfy a certain condition, processing each partition individually.\n",
    "\n",
    "**RDD Example**\n",
    "\n",
    "```python\n",
    "filtered_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(filtered_rdd.collect())  # Output: [2, 4]\n",
    "```\n",
    "\n",
    "**DataFrame Example**\n",
    "\n",
    "```python\n",
    "filtered_df = df.filter(col(\"value\") % 2 == 0)\n",
    "filtered_df.show()  # Output shows rows with even values\n",
    "```\n",
    "\n",
    "#### **flatMap()**\n",
    "-------------\n",
    "\n",
    "Similar to map(), but flatMap() allows each input element to produce zero, one, or multiple output elements, still operating within each partition independently.\n",
    "\n",
    "**RDD Example**\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([\"hello world\", \"spark transformations\"])\n",
    "flat_mapped_rdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(flat_mapped_rdd.collect())  # Output: ['hello', 'world', 'spark', 'transformations']\n",
    "```\n",
    "\n",
    "**DataFrame Example**\n",
    "\n",
    "DataFrames do not directly support flatMap(), but we can achieve similar results using explode(), especially when working with arrays.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import explode, split\n",
    "df = spark.createDataFrame([(\"hello world\",), (\"spark transformations\",)], [\"sentence\"])\n",
    "flat_mapped_df = df.withColumn(\"word\", explode(split(col(\"sentence\"), \" \")))\n",
    "flat_mapped_df.show()  # Output shows each word as a separate row\n",
    "```\n",
    "\n",
    "**2\\. Wide Transformations**\n",
    "----------------------------\n",
    "\n",
    "A **wide transformation** requires data from multiple partitions to create a single output partition. This often triggers a **shuffle**, a process where Spark redistributes data across the cluster, resulting in increased network I/O. Wide transformations are generally more resource-intensive and can impact performance due to the coordination needed across nodes.\n",
    "\n",
    "**Key Characteristics of Wide Transformations:**\n",
    "------------------------------------------------\n",
    "\n",
    "*   Data shuffle across nodes, increasing network and computation overhead.\n",
    "*   Inter-partition dependencies.\n",
    "*   Generally slower due to data redistribution.\n",
    "\n",
    "## **Examples of Wide Transformations**\n",
    "\n",
    "#### **groupByKey()**\n",
    "----------------\n",
    "\n",
    "The groupByKey() transformation groups data based on a key. It collects all values associated with each key across partitions, triggering a shuffle.\n",
    "\n",
    "**RDD Example**\n",
    "\n",
    "```python\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4)])\n",
    "grouped_rdd = rdd.groupByKey().mapValues(list)\n",
    "print(grouped_rdd.collect())  # Output: [('a', [1, 3]), ('b', [2, 4])]\n",
    "```\n",
    "\n",
    "**DataFrame Example**\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"a\", 3), (\"b\", 4)], [\"key\", \"value\"])\n",
    "grouped_df = df.groupBy(\"key\").agg({\"value\": \"collect_list\"})\n",
    "grouped_df.show()  # Output shows key-value pairs with grouped values\n",
    "```\n",
    "\n",
    "#### **reduceByKey()**\n",
    "-----------------\n",
    "\n",
    "The reduceByKey() transformation works like groupByKey() but applies a reducing function while shuffling data, optimizing data movement by aggregating values locally before the shuffle.\n",
    "\n",
    "**RDD Example**\n",
    "\n",
    "```python\n",
    "reduced_rdd = rdd.reduceByKey(lambda x, y: x + y)\n",
    "print(reduced_rdd.collect())  # Output: [('a', 4), ('b', 6)]\n",
    "```\n",
    "\n",
    "**DataFrame Example**\n",
    "\n",
    "```python\n",
    "from pyspark.sql.functions import sum\n",
    "reduced_df = df.groupBy(\"key\").agg(sum(\"value\").alias(\"sum_value\"))\n",
    "reduced_df.show()  # Output shows the sum of values for each key\n",
    "```\n",
    "\n",
    "#### **join()**\n",
    "----------\n",
    "\n",
    "The join() transformation merges two datasets based on a key, necessitating a shuffle to align matching keys across partitions.\n",
    "\n",
    "**RDD Example**\n",
    "\n",
    "```python\n",
    "rdd1 = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "rdd2 = sc.parallelize([(\"a\", 3), (\"b\", 4)])\n",
    "joined_rdd = rdd1.join(rdd2)\n",
    "print(joined_rdd.collect())  # Output: [('a', (1, 3)), ('b', (2, 4))]\n",
    "```\n",
    "\n",
    "**DataFrame Example**\n",
    "\n",
    "```python\n",
    "df1 = spark.createDataFrame([(\"a\", 1), (\"b\", 2)], [\"key\", \"value1\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 3), (\"b\", 4)], [\"key\", \"value2\"])\n",
    "joined_df = df1.join(df2, \"key\")\n",
    "joined_df.show()  # Output shows joined DataFrame based on key\n",
    "```\n",
    "\n",
    "> While typical joins in Spark trigger costly shuffles across partitions, a broadcast join optimizes performance by distributing a smaller dataset to all worker nodes, enabling local joins without network overhead — making it a powerful tool for efficient, narrow-like transformations.\n",
    "\n",
    "------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**Key Difference: Data Shuffling in Narrow vs. Wide Transformations**\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "The primary distinction between narrow and wide transformations lies in **data shuffling**. In narrow transformations, Spark processes data locally within each partition, avoiding network traffic. However, wide transformations require data to be redistributed across partitions, resulting in network I/O. For instance:\n",
    "\n",
    "*   A `map()` operation applies functions independently within each partition, while `reduceByKey()` necessitates a shuffle to ensure values with the same key are processed together.\n",
    "\n",
    "**Conclusion**\n",
    "--------------\n",
    "\n",
    "In Spark, understanding the nature of transformations — whether narrow or wide — is crucial for optimizing distributed data processing. While narrow transformations enhance performance through localized computation, wide transformations are essential for operations that require data aggregation or alignment across partitions. By strategically combining both types of transformations, you can build efficient, scalable Spark applications that make the most of distributed data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb0ea9",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2a8fd",
   "metadata": {},
   "source": [
    "# 🧠 Stateless Transformation in Apache Spark\n",
    "\n",
    "In Apache Spark, **stateless transformations** are operations that do **not depend on the previous state** of data. Each input element is processed **independently**, and the transformation does **not retain any information** about previously processed elements.\n",
    "\n",
    "A stateless transformation is an operation applied to a dataset where the processing of each element or micro-batch of data is entirely independent of any previous or subsequent elements or batches. This means the transformation does not require any \"memory\" or \"state\" from past computations to produce its output.\n",
    "\n",
    "\n",
    "## 🔍 Key Characteristics\n",
    "- **Independence**: Each input record or micro-batch is processed in isolation. The result of processing one piece of data does not depend on the results of processing any other piece of data, whether from the same batch or a previous one.\n",
    "- **No retained state**: Stateless transformations do not maintain or store any information across different processing units (e.g., RDDs in Spark Streaming DStreams or micro-batches in Spark Structured Streaming).\n",
    "- **Simplicity and efficiency**: Due to their independent nature, stateless transformations are often simpler to implement and can be highly efficient, as they do not incur the overhead of state management or consistency guarantees across time.\n",
    "- **No memory of past data**: Each record is transformed without considering others.\n",
    "- **Parallelizable**: Ideal for distributed processing across nodes.\n",
    "- **Deterministic**: Given the same input, always produces the same output.\n",
    "\n",
    "## ⚙️ Common Stateless Transformations\n",
    "\n",
    "| Transformation | Description |\n",
    "|----------------|-------------|\n",
    "| `map()`        | Applies a function to each element in the RDD or DataFrame. |\n",
    "| `filter()`     | Selects elements that satisfy a predicate. |\n",
    "| `flatMap()`    | Similar to `map()`, but can return multiple output elements for each input. |\n",
    "| `union()`      | Combines two RDDs or DataFrames. |\n",
    "| `distinct()`   | Removes duplicate elements. |\n",
    "| `select()`   | (In DataFrames/Datasets) Selects specific columns from a dataset. |\n",
    "| `where()` or `filter()`   | (In DataFrames/Datasets) Filters rows based on a condition. |\n",
    "\n",
    "In contrast, ***stateful*** transformations, such as `reduceByKeyAndWindow` in Spark Streaming or aggregations with `groupBy` and `agg` in Structured Streaming that involve time windows, require maintaining state across batches to compute results based on historical data.\n",
    "\n",
    "\n",
    "## 📦 Example in Scala\n",
    "\n",
    "```scala\n",
    "val data = sc.parallelize(Seq(1, 2, 3, 4, 5))\n",
    "val squared = data.map(x => x * x)\n",
    "squared.collect() // Output: Array(1, 4, 9, 16, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf4385",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2955dadb",
   "metadata": {},
   "source": [
    "# 🔄 Stateless vs Stateful Streaming in Apache Spark\n",
    "\n",
    "Apache Spark Streaming supports two types of data processing: **stateless** and **stateful**. These define how Spark handles data across micro-batches in a stream.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Stateless Streaming\n",
    "\n",
    "In **stateless streaming**, each micro-batch is processed **independently**. Spark does **not retain any information** from previous batches.\n",
    "\n",
    "### ✅ Characteristics\n",
    "\n",
    "- ❌ No memory of past data\n",
    "- ⚡ Fast and efficient\n",
    "- 🟢 Easy to scale and parallelize\n",
    "- 🔧 Suitable for simple transformations\n",
    "\n",
    "### 🔧 Common Stateless Operations\n",
    "\n",
    "| Operation   | Description |\n",
    "|-------------|-------------|\n",
    "| `map()`     | Applies a function to each element |\n",
    "| `filter()`  | Filters elements based on a condition |\n",
    "| `flatMap()` | Returns multiple outputs per input |\n",
    "| `reduceByKey()` | Aggregates data within a single batch |\n",
    "\n",
    "### 📦 Example\n",
    "\n",
    "```scala\n",
    "val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "val words = stream.flatMap(_.split(\" \"))\n",
    "val filtered = words.filter(_.length > 3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Stateful Streaming\n",
    "\n",
    "In **stateful streaming**, Spark maintains **state information** across batches. This enables complex operations that depend on historical data.\n",
    "\n",
    "### ✅ Characteristics\n",
    "\n",
    "- ✅ Maintains memory of previous data\n",
    "- 🧠 Enables advanced analytics (e.g., trends, aggregates)\n",
    "- 🔴 Higher resource usage\n",
    "- 🛠 Requires checkpointing for fault tolerance\n",
    "\n",
    "### 🔧 Common Stateful Operations\n",
    "\n",
    "| Operation             | Description |\n",
    "|------------------------|-------------|\n",
    "| `updateStateByKey()`   | Maintains running state per key |\n",
    "| `mapWithState()`       | Efficient state tracking |\n",
    "| `window()`             | Aggregates data over time windows |\n",
    "| `reduceByKeyAndWindow()` | Combines keys over a sliding window |\n",
    "\n",
    "\n",
    "### 📦 Example\n",
    "\n",
    "```scala\n",
    "val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
    "val pairs = stream.map(word => (word, 1))\n",
    "val runningCounts = pairs.updateStateByKey((newValues: Seq[Int], state: Option[Int]) =>\n",
    "  Some(state.getOrElse(0) + newValues.sum)\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🆚 Comparison Table\n",
    "\n",
    "| Feature               | Stateless Streaming        | Stateful Streaming         |\n",
    "|-----------------------|----------------------------|----------------------------|\n",
    "| Memory of past data   | ❌ No                      | ✅ Yes                     |\n",
    "| Complexity            | 🟢 Simple                  | 🔴 Complex                 |\n",
    "| Use cases             | Basic filtering, mapping   | Aggregations, sessions     |\n",
    "| Performance           | ⚡ Fast                    | 🐢 Slower (more overhead)  |\n",
    "| Fault tolerance       | Easier to manage           | Requires checkpointing     |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 When to Use What?\n",
    "\n",
    "- Use **stateless** for lightweight, scalable tasks.\n",
    "- Use **stateful** when tracking trends or maintaining cumulative metrics over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c35f7",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96dc8e",
   "metadata": {},
   "source": [
    "# ⚙️ Batch Output Modes in Spark\n",
    "\n",
    "Batch jobs process **static data** (already available). When writing results to a sink (like a file or database), Spark offers different **output modes**:\n",
    "\n",
    "| Mode      | Description                                                                 | Behavior Example                          |\n",
    "|-----------|-----------------------------------------------------------------------------|-------------------------------------------|\n",
    "| `append`  | ➕ Adds new data to the existing output                                      | Adds rows to a file without deleting old ones |\n",
    "| `override`| 🔄 Replaces existing output with new data                                   | Deletes old file and writes new results   |\n",
    "| `iferror` | ❌ Writes output only if the job fails (used for error handling)            | Logs error data to a file                 |\n",
    "| `ignore`  | 🙈 Skips writing output entirely                                             | Useful for debugging or dry runs          |\n",
    "\n",
    "## 📊 Diagram: Batch Output Modes\n",
    "\n",
    "```plaintext\n",
    "[Input Data] --> [Spark Batch Job] --> [Output Sink]\n",
    "                             |\n",
    "                             ├─ append   ➝ add to existing\n",
    "                             ├─ override ➝ replace existing\n",
    "                             ├─ iferror  ➝ write only on error\n",
    "                             └─ ignore   ➝ skip writing\n",
    "```\n",
    "\n",
    "# 🧪 Code Example\n",
    "\n",
    "## ✅ Batch Mode Example\n",
    "\n",
    "```scala\n",
    "df.write\n",
    "  .mode(\"append\") // or \"override\", \"ignore\", \"iferror\"\n",
    "  .csv(\"/path/to/output\")\n",
    "```\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "# 🔄 Stream Processing Output Modes in Spark\n",
    "\n",
    "Stream jobs process **real-time data** (continuous input). Spark supports these **streaming output modes**:\n",
    "\n",
    "| Mode       | Description                                                                 | Use Case Example                          |\n",
    "|------------|-----------------------------------------------------------------------------|-------------------------------------------|\n",
    "| `append`   | ➕ Writes only new rows since last trigger                                   | Logging new events                        |\n",
    "| `update`   | 🔄 Writes only rows that changed since last trigger                         | Updating metrics or counters              |\n",
    "| `complete` | 🧹 Writes the full result table every time                                   | Full aggregation (e.g., word count)       |\n",
    "\n",
    "## 🔁 Diagram: Stream Output Modes\n",
    " \n",
    "```plaintext\n",
    "[Streaming Source] --> [Spark Structured Streaming] --> [Output Sink]\n",
    "                                 |\n",
    "                                 ├─ append   ➝ new rows only\n",
    "                                 ├─ update   ➝ changed rows only\n",
    "                                 └─ complete ➝ entire result table\n",
    "```\n",
    "\n",
    "# 🧪 Code Example\n",
    "\n",
    "## 🔄 Streaming Mode Example\n",
    "\n",
    "```scala\n",
    "val query = df.writeStream\n",
    "  .outputMode(\"append\") // or \"update\", \"complete\"\n",
    "  .format(\"console\")\n",
    "  .start()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74686ef",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0766f00",
   "metadata": {},
   "source": [
    "#### • Streaming refers to the continuous processing of incoming data in real-time or near-real-time. Unlike batch processing, which works on a fixed dataset, streaming deals with infinite or unbounded data (e.g., logs, sensor data, or user clicks) that keeps arriving.\n",
    "#### •Think of it like water flowing through a pipe-you don’t wait for all the water to arrive before acting; you work wait it as if lows.\n",
    "### Stream = unbounded Table\n",
    "#### • A stream is treated as a table that is constantly growing.\n",
    "#### • New data is like new rows being appended to the table.\n",
    "#### • It allows you to query this growing table as if it were a static table.\n",
    "#### • It keeps updating the results as new data arrives.\n",
    "#### • Internally, it maintains the state, figures out what changed, and updates what's needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ca8df9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "375bd68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "#\n",
    "#\n",
    "_builder =\t(\tSparkSession.builder.master('local[1]') \\\n",
    "\t\t\t\t.appName('pyspark-deltalake-local-testing') \\\n",
    "\t\t\t\t.config(\t'spark.sql.extensions'\n",
    "\t\t\t\t\t\t,\t'io.delta.sql.DeltaSparkSessionExtension')\n",
    "\t\t\t\t.config(\t'spark.sql.catalog.spark_catalog'\n",
    "\t\t\t\t\t\t,\t'org.apache.spark.sql.delta.catalog.DeltaCatalog'))\n",
    "#\n",
    "_spark = configure_spark_with_delta_pip(_builder).enableHiveSupport().getOrCreate()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "289f87b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ROBOTINA:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-deltalake-local-testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2cc58a40a90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark\n",
    "#\n",
    "# try to stop?\n",
    "### _spark.stop()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7dee8",
   "metadata": {},
   "source": [
    "### READ JSON DATA (BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4963a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df = _spark.read.format('json') \\\n",
    "\t\t\t.option('inferschema', True) \\\n",
    "\t\t\t.option('multiLine',True) \\\n",
    "\t\t\t.load('file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Streaming/day1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2470c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------+-------------------------------------------------------------------+----------------------------------------------+--------+----------------------+--------------------+\n",
      "|customer                                                     |items                                                              |metadata                                      |order_id|payment               |timestamp           |\n",
      "+-------------------------------------------------------------+-------------------------------------------------------------------+----------------------------------------------+--------+----------------------+--------------------+\n",
      "|{{Toronto, Canada, M5H 2N2}, 501, john@example.com, John Doe}|[{I100, 25.99, Wireless Mouse, 2}, {I101, 15.49, USB-C Adapter, 1}]|[{campaign, back_to_school}, {channel, email}]|ORD1001 |{Credit Card, TXN7890}|2025-06-01T10:15:00Z|\n",
      "+-------------------------------------------------------------+-------------------------------------------------------------------+----------------------------------------------+--------+----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acd5cec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer: struct (nullable = true)\n",
      " |    |-- address: struct (nullable = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- country: string (nullable = true)\n",
      " |    |    |-- postal_code: string (nullable = true)\n",
      " |    |-- customer_id: long (nullable = true)\n",
      " |    |-- email: string (nullable = true)\n",
      " |    |-- name: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- price: double (nullable = true)\n",
      " |    |    |-- product_name: string (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- metadata: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- key: string (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment: struct (nullable = true)\n",
      " |    |-- method: string (nullable = true)\n",
      " |    |-- transaction_id: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb366c0c",
   "metadata": {},
   "source": [
    "### let's Parse the Schema for us to create our own schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83c9d5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('customer', StructType([StructField('address', StructType([StructField('city', StringType(), True), StructField('country', StringType(), True), StructField('postal_code', StringType(), True)]), True), StructField('customer_id', LongType(), True), StructField('email', StringType(), True), StructField('name', StringType(), True)]), True), StructField('items', ArrayType(StructType([StructField('item_id', StringType(), True), StructField('price', DoubleType(), True), StructField('product_name', StringType(), True), StructField('quantity', LongType(), True)]), True), True), StructField('metadata', ArrayType(StructType([StructField('key', StringType(), True), StructField('value', StringType(), True)]), True), True), StructField('order_id', StringType(), True), StructField('payment', StructType([StructField('method', StringType(), True), StructField('transaction_id', StringType(), True)]), True), StructField('timestamp', StringType(), True)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "inferred_schema = _df.schema\n",
    "schema_json_string = inferred_schema.json()\n",
    "## ## print(json.dumps(json.loads(schema_json_string), indent=2))\n",
    "#\n",
    "from pyspark.sql.types import StructType\n",
    "# Assuming schema_json_string holds the JSON representation of your schema\n",
    "recreated_schema = StructType.fromJson(json.loads(schema_json_string))\n",
    "print(recreated_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "385898ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+-------+-------+--------+--------------------+\n",
      "|customer_id|name    |city   |country|order_id|timestamp           |\n",
      "+-----------+--------+-------+-------+--------+--------------------+\n",
      "|501        |John Doe|Toronto|Canada |ORD1001 |2025-06-01T10:15:00Z|\n",
      "+-----------+--------+-------+-------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.select('customer.customer_id','customer.name','customer.address.city','customer.address.country','order_id','timestamp').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1836f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------+--------+-------+-------+--------+--------------------+-------+----------+\n",
      "|items                           |customer_id|name    |city   |country|order_id|timestamp           |item_id|item_price|\n",
      "+--------------------------------+-----------+--------+-------+-------+--------+--------------------+-------+----------+\n",
      "|{I100, 25.99, Wireless Mouse, 2}|501        |John Doe|Toronto|Canada |ORD1001 |2025-06-01T10:15:00Z|I100   |25.99     |\n",
      "|{I101, 15.49, USB-C Adapter, 1} |501        |John Doe|Toronto|Canada |ORD1001 |2025-06-01T10:15:00Z|I101   |15.49     |\n",
      "+--------------------------------+-----------+--------+-------+-------+--------+--------------------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "#\n",
    "_df.select('items','customer.customer_id','customer.name','customer.address.city','customer.address.country','order_id','timestamp') \\\n",
    "\t.withColumn('items',explode_outer(col('items'))) \\\n",
    "\t.withColumn('item_id', col('items.item_id')) \\\n",
    "\t.withColumn('item_price', col('items.price')) \\\n",
    "\t.select('*') \\\n",
    "\t.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56f9d301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+------------+--------------+\n",
      "|order_id|timestamp           |metadata_key|metadata_value|\n",
      "+--------+--------------------+------------+--------------+\n",
      "|ORD1001 |2025-06-01T10:15:00Z|campaign    |back_to_school|\n",
      "|ORD1001 |2025-06-01T10:15:00Z|channel     |email         |\n",
      "+--------+--------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "#\n",
    "_df.select('*') \\\n",
    "\t.withColumn('metadata',explode_outer(col('metadata'))) \\\n",
    "\t.withColumn('metadata_key', col('metadata.key')) \\\n",
    "\t.withColumn('metadata_value', col('metadata.value')) \\\n",
    "\t.drop(col('metadata')) \\\n",
    "\t.select('*') \\\n",
    "\t.select(['order_id','timestamp','metadata_key','metadata_value']) \\\n",
    "\t.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793a46c2",
   "metadata": {},
   "source": [
    "# Let's Read Streamimg Data 😁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "805253fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## instead using .option('inferschema', True), we'll use:\n",
    "_spark.conf.set('spark.sql.streaming.schemaInference', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc160978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------+-----------+----------------+--------+-------+----------+--------------+--------+--------+--------------+--------------+--------------------+\n",
      "|city   |country|postal_code|customer_id|email           |name    |item_id|item_price|product_name  |quantity|order_id|payment_method|transaction_id|timestamp           |\n",
      "+-------+-------+-----------+-----------+----------------+--------+-------+----------+--------------+--------+--------+--------------+--------------+--------------------+\n",
      "|Toronto|Canada |M5H 2N2    |501        |john@example.com|John Doe|I100   |25.99     |Wireless Mouse|2       |ORD1001 |Credit Card   |TXN7890       |2025-06-01T10:15:00Z|\n",
      "|Toronto|Canada |M5H 2N2    |501        |john@example.com|John Doe|I100   |25.99     |Wireless Mouse|2       |ORD1001 |Credit Card   |TXN7890       |2025-06-01T10:15:00Z|\n",
      "|Toronto|Canada |M5H 2N2    |501        |john@example.com|John Doe|I101   |15.49     |USB-C Adapter |1       |ORD1001 |Credit Card   |TXN7890       |2025-06-01T10:15:00Z|\n",
      "|Toronto|Canada |M5H 2N2    |501        |john@example.com|John Doe|I101   |15.49     |USB-C Adapter |1       |ORD1001 |Credit Card   |TXN7890       |2025-06-01T10:15:00Z|\n",
      "+-------+-------+-----------+-----------+----------------+--------+-------+----------+--------------+--------+--------+--------------+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "#\n",
    "_jsonSchema = \\\n",
    "StructType([StructField('customer'\n",
    "\t\t\t,StructType([StructField('address'\n",
    "\t\t\t\t,StructType([StructField('city',StringType(),True)\n",
    "\t\t\t\t\t\t\t,StructField('country',StringType(),True)\n",
    "\t\t\t\t\t\t\t,StructField('postal_code',StringType(),True)]),True)\n",
    "\t\t\t\t,StructField('customer_id',LongType(),True)\n",
    "\t\t\t\t,StructField('email',StringType(),True)\n",
    "\t\t\t\t,StructField('name',StringType(),True)]),True)\n",
    "\t\t\t,StructField('items'\n",
    "\t\t\t\t\t,ArrayType(StructType(\n",
    "\t\t\t\t\t\t\t\t[\tStructField('item_id',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('price',DoubleType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('product_name',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('quantity',LongType(),True)]),True),True)\n",
    "\t\t\t,StructField('metadata'\n",
    "\t\t\t\t\t,ArrayType(StructType(\n",
    "\t\t\t\t\t\t\t\t[\tStructField('key',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('value',StringType(),True)]),True),True)\n",
    "\t\t\t,StructField('order_id',StringType(),True)\n",
    "\t\t\t,StructField('payment',StructType(\n",
    "\t\t\t\t\t\t\t\t\t\t[StructField('method',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t\t,StructField('transaction_id',StringType(),True)]),True)\n",
    "\t\t\t,StructField('timestamp',StringType(),True)])\n",
    "#\n",
    "_df = _spark.read.format('json') \\\n",
    "\t\t\t.option('inferschema', True) \\\n",
    "\t\t\t.option('multiLine',True) \\\n",
    "\t\t\t.load('file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Streaming/day1.json')\n",
    "#\n",
    "_dfToWrite = _df.select('*') \\\n",
    "\t.withColumn('items',explode_outer(col('items'))) \\\n",
    "\t.withColumn('item_id', col('items.item_id')) \\\n",
    "\t.withColumn('item_price', col('items.price')) \\\n",
    "\t.withColumn('product_name', col('items.product_name')) \\\n",
    "\t.withColumn('quantity', col('items.quantity')) \\\n",
    "\t.drop(col('items')) \\\n",
    "\t.withColumn('metadata',explode_outer(col('metadata'))) \\\n",
    "\t.withColumn('metadata_key', col('metadata.key')) \\\n",
    "\t.withColumn('metadata_value', col('metadata.value')) \\\n",
    "\t.drop(col('metadata')) \\\n",
    "\t.withColumn('payment_method', col('payment.method')) \\\n",
    "\t.withColumn('transaction_id', col('payment.transaction_id')) \\\n",
    "\t.drop(col('payment')) \\\n",
    "\t.select(\t[\n",
    "\t\t\t\t\t\t'customer.address.city'\n",
    "\t\t\t\t\t,\t'customer.address.country'\n",
    "\t\t\t\t\t,\t'customer.address.postal_code'\n",
    "\t\t\t\t\t,\t'customer.customer_id'\n",
    "\t\t\t\t\t,\t'customer.email'\n",
    "\t\t\t\t\t,\t'customer.name'\n",
    "\t\t\t\t\t,\t'item_id'\n",
    "\t\t\t\t\t,\t'item_price'\n",
    "\t\t\t\t\t,\t'product_name'\n",
    "\t\t\t\t\t,\t'quantity'\n",
    "\t\t\t\t\t,\t'order_id'\n",
    "\t\t\t\t\t,\t'payment_method'\n",
    "\t\t\t\t\t,\t'transaction_id'\n",
    "\t\t\t\t\t,\t'timestamp'\n",
    "\t\t\t\t])\n",
    "#\n",
    "_dfToWrite.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6e5816",
   "metadata": {},
   "source": [
    "#### for error:\n",
    "\n",
    "```plaintext\n",
    "[INFINITE_STREAMING_TRIGGER_NOT_SUPPORTED] Trigger type ProcessingTime is not supported for this cluster type.\n",
    "Use a different trigger type e.g. AvailableNow, Once. SQLSTATE: 0A000\n",
    "```\n",
    "\n",
    "#### you need to use an alternative trigger type that is compatible with your cluster. The error message specifically suggests `AvailableNow` or `Once`.\n",
    "\n",
    "- `AvailableNow`: This trigger processes all data that is available at the start of the query and then stops. If new data arrives later, it will not be processed until the next time the query is restarted with AvailableNow. This is suitable for incremental batch processing where you want to process all currently available data in a single run.\n",
    "\n",
    "```python\n",
    ".trigger(availableNow=True)\n",
    "```\n",
    "\n",
    "- `Once`: Similar to `AvailableNow`, this trigger processes a single batch of available data and then stops. It is considered deprecated in favor of `AvailableNow` for most use cases, but it can still be used if explicitly required for a single-batch execution.\n",
    "\n",
    "```python\n",
    ".trigger(once=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f927b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#\\nfrom pyspark.sql.functions import *\\nfrom pyspark.sql.window import *\\nfrom pyspark.sql.types import *\\n#\\n_jsonSchema = StructType([StructField('customer'\\n\\t\\t\\t,StructType([StructField('address'\\n\\t\\t\\t\\t,StructType([StructField('city',StringType(),True)\\n\\t\\t\\t\\t\\t\\t\\t,StructField('country',StringType(),True)\\n\\t\\t\\t\\t\\t\\t\\t,StructField('postal_code',StringType(),True)]),True)\\n\\t\\t\\t\\t,StructField('customer_id',LongType(),True)\\n\\t\\t\\t\\t,StructField('email',StringType(),True)\\n\\t\\t\\t\\t,StructField('name',StringType(),True)]),True)\\n\\t\\t\\t,StructField('items'\\n\\t\\t\\t\\t\\t,ArrayType(StructType(\\n\\t\\t\\t\\t\\t\\t\\t\\t[\\tStructField('item_id',StringType(),True)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t,StructField('price',DoubleType(),True)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t,StructField('product_name',StringType(),True)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t,StructField('quantity',LongType(),True)]),True),True)\\n\\t\\t\\t,StructField('metadata'\\n\\t\\t\\t\\t\\t,ArrayType(StructType(\\n\\t\\t\\t\\t\\t\\t\\t\\t[\\tStructField('key',StringType(),True)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t,StructField('value',StringType(),True)]),True),True)\\n\\t\\t\\t,StructField('order_id',StringType(),True)\\n\\t\\t\\t,StructField('payment',StructType(\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t[StructField('method',StringType(),True)\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t,StructField('transaction_id',StringType(),True)]),True)\\n\\t\\t\\t,StructField('timestamp',StringType(),True)])\\n#\\n#\\n_df = spark.readStream.format('json') \\t\\t\\t.option('multiLine',True) \\t\\t\\t.schema(_jsonSchema) \\t\\t\\t.load('/Volumes/workspace/stream/streamingvolume/jsonsource')\\n#\\n_dfToWrite = _df.select('*') \\t.withColumn('items',explode_outer(col('items'))) \\t.withColumn('item_id', col('items.item_id')) \\t.withColumn('item_price', col('items.price')) \\t.withColumn('product_name', col('items.product_name')) \\t.withColumn('quantity', col('items.quantity')) \\t.drop(col('items')) \\t.withColumn('metadata',explode_outer(col('metadata'))) \\t.withColumn('metadata_key', col('metadata.key')) \\t.withColumn('metadata_value', col('metadata.value')) \\t.drop(col('metadata')) \\t.withColumn('payment_method', col('payment.method')) \\t.withColumn('transaction_id', col('payment.transaction_id')) \\t.drop(col('payment')) \\t.select(\\t[\\n\\t\\t\\t\\t\\t\\t'customer.address.city'\\n\\t\\t\\t\\t\\t,\\t'customer.address.country'\\n\\t\\t\\t\\t\\t,\\t'customer.address.postal_code'\\n\\t\\t\\t\\t\\t,\\t'customer.customer_id'\\n\\t\\t\\t\\t\\t,\\t'customer.email'\\n\\t\\t\\t\\t\\t,\\t'customer.name'\\n\\t\\t\\t\\t\\t,\\t'item_id'\\n\\t\\t\\t\\t\\t,\\t'item_price'\\n\\t\\t\\t\\t\\t,\\t'product_name'\\n\\t\\t\\t\\t\\t,\\t'quantity'\\n\\t\\t\\t\\t\\t,\\t'order_id'\\n\\t\\t\\t\\t\\t,\\t'payment_method'\\n\\t\\t\\t\\t\\t,\\t'transaction_id'\\n\\t\\t\\t\\t\\t,\\t'timestamp'\\n\\t\\t\\t\\t])\\n#\\n_dfToWrite.writeStream.format('delta') \\t\\t\\t.outputMode('append') \\t\\t\\t.option('path','/Volumes/workspace/stream/streamingvolume/jsonsink/Data') \\t\\t\\t.option('checkpointLocation','/Volumes/workspace/stream/streamingvolume/jsonsink/Checkpoint') \\t\\t\\t.start()\\n#\\n\\t\\t\\t\\n\\t\\t\\t\\n\\t\\t\\t\\n\\n\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.types import *\n",
    "#\n",
    "_jsonSchema = \\\n",
    "StructType([StructField('customer'\n",
    "\t\t\t,StructType([StructField('address'\n",
    "\t\t\t\t,StructType([StructField('city',StringType(),True)\n",
    "\t\t\t\t\t\t\t,StructField('country',StringType(),True)\n",
    "\t\t\t\t\t\t\t,StructField('postal_code',StringType(),True)]),True)\n",
    "\t\t\t\t,StructField('customer_id',LongType(),True)\n",
    "\t\t\t\t,StructField('email',StringType(),True)\n",
    "\t\t\t\t,StructField('name',StringType(),True)]),True)\n",
    "\t\t\t,StructField('items'\n",
    "\t\t\t\t\t,ArrayType(StructType(\n",
    "\t\t\t\t\t\t\t\t[\tStructField('item_id',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('price',DoubleType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('product_name',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('quantity',LongType(),True)]),True),True)\n",
    "\t\t\t,StructField('metadata'\n",
    "\t\t\t\t\t,ArrayType(StructType(\n",
    "\t\t\t\t\t\t\t\t[\tStructField('key',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t,StructField('value',StringType(),True)]),True),True)\n",
    "\t\t\t,StructField('order_id',StringType(),True)\n",
    "\t\t\t,StructField('payment',StructType(\n",
    "\t\t\t\t\t\t\t\t\t\t[StructField('method',StringType(),True)\n",
    "\t\t\t\t\t\t\t\t\t\t,StructField('transaction_id',StringType(),True)]),True)\n",
    "\t\t\t,StructField('timestamp',StringType(),True)])\n",
    "#\n",
    "#\n",
    "_df = spark.readStream.format('json') \\\n",
    "\t\t\t.option('multiLine',True) \\\n",
    "\t\t\t.schema(_jsonSchema) \\\n",
    "\t\t\t.load('/Volumes/workspace/stream/streamingvolume/jsonsource')\n",
    "#\n",
    "_dfToWrite = _df.select('*') \\\n",
    "\t.withColumn('items',explode_outer(col('items'))) \\\n",
    "\t.withColumn('item_id', col('items.item_id')) \\\n",
    "\t.withColumn('item_price', col('items.price')) \\\n",
    "\t.withColumn('product_name', col('items.product_name')) \\\n",
    "\t.withColumn('quantity', col('items.quantity')) \\\n",
    "\t.drop(col('items')) \\\n",
    "\t.withColumn('metadata',explode_outer(col('metadata'))) \\\n",
    "\t.withColumn('metadata_key', col('metadata.key')) \\\n",
    "\t.withColumn('metadata_value', col('metadata.value')) \\\n",
    "\t.drop(col('metadata')) \\\n",
    "\t.withColumn('payment_method', col('payment.method')) \\\n",
    "\t.withColumn('transaction_id', col('payment.transaction_id')) \\\n",
    "\t.drop(col('payment')) \\\n",
    "\t.select(\t[\n",
    "\t\t\t\t\t\t'customer.address.city'\n",
    "\t\t\t\t\t,\t'customer.address.country'\n",
    "\t\t\t\t\t,\t'customer.address.postal_code'\n",
    "\t\t\t\t\t,\t'customer.customer_id'\n",
    "\t\t\t\t\t,\t'customer.email'\n",
    "\t\t\t\t\t,\t'customer.name'\n",
    "\t\t\t\t\t,\t'item_id'\n",
    "\t\t\t\t\t,\t'item_price'\n",
    "\t\t\t\t\t,\t'product_name'\n",
    "\t\t\t\t\t,\t'quantity'\n",
    "\t\t\t\t\t,\t'order_id'\n",
    "\t\t\t\t\t,\t'payment_method'\n",
    "\t\t\t\t\t,\t'transaction_id'\n",
    "\t\t\t\t\t,\t'timestamp'\n",
    "\t\t\t\t])\n",
    "#\n",
    "_dfToWrite.writeStream.format('delta') \\\n",
    "\t\t\t.trigger(once=True) \\\n",
    "\t\t\t.outputMode('append') \\\n",
    "\t\t\t.option('path','/Volumes/workspace/stream/streamingvolume/jsonsink/Data') \\\n",
    "\t\t\t.option('checkpointLocation','/Volumes/workspace/stream/streamingvolume/jsonsink/Checkpoint') \\\n",
    "\t\t\t.start()\n",
    "#\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c54c8a8",
   "metadata": {},
   "source": [
    "```sql\n",
    "%sql\n",
    "SELECT * FROM delta.`/Volumes/workspace/stream/streamingvolume/jsonsink/Data/`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264c314d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4be11b",
   "metadata": {},
   "source": [
    "## 🔥 PySpark `writeStream.trigger()` — Simple Guide with All Trigger Types\n",
    "### 🧠 What is `trigger` in `writeStream`?\n",
    "\n",
    "In PySpark Structured Streaming, a **trigger** controls **when** or **how often** the streaming job checks for new data and processes it.\n",
    "\n",
    "Think of it like setting a timer ⏱️:\n",
    "- Every few seconds, PySpark checks for new data and processes it.\n",
    "- You decide how often PySpark should wake up and do its job.\n",
    "- You control this timing using `.trigger(...)`.\n",
    "- Different triggers = different behaviors.\n",
    "\n",
    "\n",
    "## 🔁 Trigger Types\n",
    "\n",
    "| Trigger Type        | Description                                                      | Stops Automatically? | Example                          |\n",
    "|---------------------|------------------------------------------------------------------|-----------------------|----------------------------------|\n",
    "| `ProcessingTime`    | Runs at fixed intervals (e.g. every 10 seconds)                  | ❌                    | Every 10 seconds                 |\n",
    "| `Once`              | Runs **once** and stops (like a mini batch job)                  | ✅                    | Good for batch-like streaming    |\n",
    "| `Continuous`        | *Row by Row*, Runs continuously with low latency (experimental)  | ❌                    | Every 1 second (near real-time)  |\n",
    "| `AvailableNow`      | Processes all available data in batches, then stops              | ✅                    ||\n",
    "\n",
    "## 📊 Diagram: How Trigger Works\n",
    "\n",
    "```plaintext\n",
    "+-------------------+       +-------------------+       +-------------------+\n",
    "| New Data Arrives  | --->  | Trigger Fires ⏰  | ---> | Process & Write 🔄|\n",
    "+-------------------+       +-------------------+       +-------------------+\n",
    "        ↑                          ↑                            ↑\n",
    "        |                          |                            |\n",
    "     Every X sec               Based on Trigger           Output Sink (e.g. file)\n",
    "```\n",
    "\n",
    "## 🐍 Python Examples\n",
    "\n",
    "### 1. ⏱️ Trigger Every 10 Seconds\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 2. 🧼 Trigger Once (like batch)\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .trigger(once=True) \\\n",
    "    .option(\"path\", \"/output/path\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 3. ⚡ Continuous Trigger (experimental)\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(continuous=\"1 second\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### 4. 📦 AvailableNow Trigger\n",
    "\n",
    "```python\n",
    "query = df.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .option(\"path\", \"/output/path\") \\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- `trigger` controls **when** streaming jobs run.\n",
    "- Use `.trigger(processingTime=\"X\")` for regular intervals.\n",
    "- Use `.trigger(once=True)` for one-time execution.\n",
    "- Use `.trigger(continuous=\"X\")` **row by row**. for near real-time and low-latency streaming (experimental).\n",
    "- Use `.trigger(availableNow=True)` to process all current data and stop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c302edd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27ff6ce2",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a5a6a",
   "metadata": {},
   "source": [
    "# 🧱 What Is a Boundary Stage in Spark?\n",
    "\n",
    "In Apache Spark, a **boundary stage** (also called a **stage boundary** or **wide dependency**) occurs when an operation requires ***shuffling data across partitions***. This typically happens during **wide transformations**, where data from multiple partitions must be exchanged or grouped. Boundary stages are performance-critical because they involve disk I/O, network transfer, and sorting—making them much more expensive than narrow transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae5b1bb",
   "metadata": {},
   "source": [
    "### ⚡ Spark Operations: Boundary vs Non-Boundary Stages\n",
    "\n",
    "| **Spark Operation**     | **Boundary Stage?** | **Type**             | **Simple Explanation**                                                                 |\n",
    "|-------------------------|---------------------|----------------------|----------------------------------------------------------------------------------------|\n",
    "| `map()`                 | ❌ No               | Narrow Transformation | Applies a function to each element. No data movement between partitions.               |\n",
    "| `filter()`              | ❌ No               | Narrow Transformation | Keeps elements that match a condition. No shuffle.                                     |\n",
    "| `flatMap()`             | ❌ No               | Narrow Transformation | Similar to `map`, but can return multiple outputs per input. No shuffle.               |\n",
    "| `union()`               | ❌ No               | Narrow Transformation | Combines two datasets. No shuffle unless followed by other wide ops.                   |\n",
    "| `sample()`              | ❌ No               | Narrow Transformation | Randomly samples elements. No shuffle.                                                 |\n",
    "| `coalesce()`            | ❌ No               | Narrow Transformation | Reduces number of partitions **without shuffle**. Efficient for downsampling.          |\n",
    "| `repartition()`         | ✅ Yes              | Wide Transformation   | Redistributes data across partitions **with shuffle**. Costly but useful for balancing.|\n",
    "| `groupByKey()`          | ✅ Yes              | Wide Transformation   | Groups data by key. Requires shuffle to bring same keys together.                      |\n",
    "| `reduceByKey()`         | ✅ Yes              | Wide Transformation   | Combines values by key **with pre-aggregation**, reducing shuffle cost.                |\n",
    "| `join()`                | ✅ Yes              | Wide Transformation   | Combines datasets by key. Requires shuffle to align keys.                              |\n",
    "| `distinct()`            | ✅ Yes              | Wide Transformation   | Removes duplicates. Requires shuffle to compare across partitions.                     |\n",
    "| `sortBy()`              | ✅ Yes              | Wide Transformation   | Sorts data. Requires shuffle to order globally.                                        |\n",
    "| `aggregateByKey()`      | ✅ Yes              | Wide Transformation   | Aggregates values by key. Similar to `reduceByKey`, but more flexible.                 |\n",
    "| `cogroup()`             | ✅ Yes              | Wide Transformation   | Groups multiple RDDs by key. Requires shuffle.                                         |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Quick Tip:\n",
    "- **Narrow transformations**: Data stays in its partition. Fast and efficient.\n",
    "- **Wide transformations**: Data moves across partitions. Triggers a new stage and shuffle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16953ab0",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4861ad5",
   "metadata": {},
   "source": [
    "### ⚙️ Spark Runtime Processes & Behaviors\n",
    "\n",
    "| **Process / Behavior**     | **Type**              | **Triggers / Causes**                                      | **Generates Boundary Stage?** | **Simple Explanation**                                                                 |\n",
    "|----------------------------|-----------------------|-------------------------------------------------------------|-------------------------------|----------------------------------------------------------------------------------------|\n",
    "| Shuffling                  | Performance Overhead  | Wide transformations like `groupByKey`, `join`, `sortBy`    | ✅ Yes                        | Redistributes data across partitions. Costly but necessary for certain operations.     |\n",
    "| Caching / Persistence      | Optimization          | Manual call to `cache()` or `persist()`                     | ❌ No                         | Stores intermediate results in memory or disk to avoid recomputation.                 |\n",
    "| Execution Failure          | Error State           | Code bugs, data issues, resource limits                     | ❌ No                         | Task or stage fails during execution. May retry depending on fault tolerance.         |\n",
    "| Job Delegation             | Scheduling Mechanism  | SparkContext submits job to DAG Scheduler                   | ❌ No                         | Breaks job into stages and tasks, then delegates to Task Scheduler for execution.     |\n",
    "| Application Failure        | Critical Error        | Driver crash, out-of-memory, unhandled exceptions           | ❌ No                         | Entire Spark app stops. Requires restart or debugging.                                |\n",
    "| Task Retry                 | Fault Tolerance       | Transient errors, executor loss                             | ❌ No                         | Spark retries failed tasks up to a configured limit.                                   |\n",
    "| Speculative Execution      | Optimization          | Straggler tasks detected                                    | ❌ No                         | Launches duplicate tasks to speed up slow-running ones.                               |\n",
    "| Stage Completion           | Execution Milestone   | All tasks in a stage finish successfully                    | ✅ Yes                        | Marks boundary for next stage. Triggers shuffle if needed.                            |\n",
    "| Job Completion             | Finalization          | All stages of a job complete                                | ❌ No                         | Results are returned to driver or written to storage.                                 |\n",
    "| Resource Allocation        | Cluster Management    | YARN, Mesos, or Kubernetes assigns executors                | ❌ No                         | Determines how many resources (CPU, memory) Spark can use.                            |\n",
    "| Broadcast Variables        | Optimization          | Used in joins or lookups with small datasets                | ❌ No                         | Sends read-only data to all executors to avoid repeated transmission.                 |\n",
    "| Accumulators               | Monitoring / Metrics  | Used for counters or debugging                              | ❌ No                         | Variables that aggregate values across tasks. Not reliable for logic control.         |\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Quick Tip:\n",
    "- Boundary stages are triggered by **shuffles**, which are expensive—so minimize them when possible.\n",
    "- Use **caching** and **broadcast variables** to optimize performance and reduce recomputation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90db9a2c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
