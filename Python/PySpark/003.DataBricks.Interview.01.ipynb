{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37805f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.youtube.com/watch?v=njE89SwtPYI&t=02h33m33s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e40b20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+--------+----------+-------+---+-----+----+-------------+------------------------------------+--------------+----------+\n",
      "|Branch_ID|Dealer_ID|Model_ID|Revenue |Units_Sold|Date_ID|Day|Month|Year|BranchName   |DealerName                          |model_category|RevPerUnit|\n",
      "+---------+---------+--------+--------+----------+-------+---+-----+----+-------------+------------------------------------+--------------+----------+\n",
      "|BR0074   |DLR0215  |Toy-M195|3163004 |2         |DT00045|19 |1    |2017|Anadol Motors|Wanderer Motors                     |Toy           |1581502.0 |\n",
      "|BR0075   |DLR0195  |Mah-M165|21878175|3         |DT00046|23 |9    |2017|Anadol Motors|Tornado Motors                      |Mah           |7292725.0 |\n",
      "|BR0077   |DLR0155  |Toy-M105|9867107 |1         |DT00046|31 |8    |2017|Anadol Motors|Piaggio Motors                      |Toy           |9867107.0 |\n",
      "|BR0078   |DLR0135  |Hyu-M75 |4583883 |3         |DT00047|5  |5    |2017|Anadol Motors|Morgan Motors                       |Hyu           |1527961.0 |\n",
      "|BR0080   |DLR0095  |Ram-M15 |14383990|2         |DT00048|11 |9    |2017|Anadol Motors|Isuzu Motors                        |Ram           |7191995.0 |\n",
      "|BR0081   |DLR0009  |Dod-M9  |24665256|3         |DT00048|9  |1    |2017|Ariel Motors |Ariel Motors                        |Dod           |8221752.0 |\n",
      "|BR0081   |DLR0076  |Nis-M263|2354637 |3         |DT00049|16 |5    |2017|Ariel Motors |Healey Motors                       |Nis           |784879.0  |\n",
      "|BR0082   |DLR0256  |Vol-M256|2563134 |3         |DT00049|13 |9    |2017|Ariel Motors |Ariel Motors                        |Vol           |854378.0  |\n",
      "|BR0083   |DLR0236  |For-M226|5756780 |1         |DT00050|18 |5    |2017|Ariel Motors |Cadillac Automobile Company Building|For           |5756780.0 |\n",
      "|BR0084   |DLR0216  |Toy-M196|4317585 |1         |DT00050|20 |1    |2017|Ariel Motors |Maybach  Motors                     |Toy           |4317585.0 |\n",
      "+---------+---------+--------+--------+----------+-------+---+-----+----+-------------+------------------------------------+--------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Set-ExecutionPolicy Bypass -Scope Process\n",
    "#\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "#\n",
    "#\n",
    "#### _spark = SparkSession.builder.master('local[1]').getOrCreate()\n",
    "#\n",
    "#\n",
    "#\t\t\t\t.config(\t'spark.sql.catalog.spark_catalog'\n",
    "#\t\t\t\t\t\t,\t'io.unitycatalog.spark.UCSingleCatalog') \\\n",
    "#\t\t\t\t.config(\t'spark.sql.catalog.unity'\n",
    "#\t\t\t\t\t\t,\t'io.unitycatalog.spark.UCSingleCatalog') \\\n",
    "#\t\t\t\t.config(\t'spark.sql.defaultCatalog'\n",
    "#\t\t\t\t\t\t,\t'unity') \\\n",
    "#\t\t\t\t.config(\t'spark.sql.catalog.unity.uri'\n",
    "#\t\t\t\t\t\t,\t'http://localhost:4041') \\\n",
    "#\n",
    "_builder =\t(\tSparkSession.builder.master('local[1]') \\\n",
    "\t\t\t\t.appName('pyspark-deltalake-local-testing') \\\n",
    "\t\t\t\t.config(\t'spark.sql.extensions'\n",
    "\t\t\t\t\t\t,\t'io.delta.sql.DeltaSparkSessionExtension')\n",
    "\t\t\t\t.config(\t'spark.sql.catalog.spark_catalog'\n",
    "\t\t\t\t\t\t,\t'org.apache.spark.sql.delta.catalog.DeltaCatalog'))\n",
    "#\n",
    "_spark = configure_spark_with_delta_pip(_builder).enableHiveSupport().getOrCreate()\n",
    "#\n",
    "#\n",
    "#\n",
    "### READ\n",
    "_demoDataFrame = _spark.read \\\n",
    "\t.option('inferSchema', True) \\\n",
    "\t.option('header',True) \\\n",
    "\t.format('parquet') \\\n",
    "\t.load('file:///C:/Users/Administrator/Documents/Code/Python/PySpark/demo.parquet')\n",
    "#\n",
    "_demoDataFrame.show(10, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b151ed7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[1]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-deltalake-local-testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d37f755290>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d678d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "## ## drop if require to delete from catalog\n",
    "#_query = 'DROP TABLE CarDealers'\n",
    "#_spark.sql(_query)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab56a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n%sql\\nCREATE TABLE db_catalog_name.db_catalog_schema_name.CarDealers\\nUSING DELTA\\nLOCATION 'abfss://destination_name@db_locationDataLake_name.dbfs.core.windows.net/parquetDataFolderName'\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "### WRITE as TABLE-DELTA\n",
    "#\n",
    "_path = 'file:///C:/Users/Administrator/Documents/Code/Python/PySpark/MyDeltaLake/CarDealers/'\n",
    "#\n",
    "_demoDataFrame.write \\\n",
    "\t\t\t\t.mode('overwrite') \\\n",
    "\t\t\t\t.format('delta') \\\n",
    "\t\t\t\t.option('path',_path) \\\n",
    "\t\t\t\t.saveAsTable(\t\\\n",
    "\t\t\t\t\t\t\t\t'CarDealers'\t\\\n",
    "\t\t\t\t\t\t\t,\tpath\t=\t_path\n",
    "\t\t\t\t\t\t\t,\tformat\t=\t'delta'\t\\\n",
    "\t\t\t\t\t\t\t,\tmode\t=\t'overwrite')\n",
    "#\n",
    "\"\"\"\n",
    "%sql\n",
    "CREATE TABLE db_catalog_name.db_catalog_schema_name.CarDealers\n",
    "USING DELTA\n",
    "LOCATION 'abfss://destination_name@db_locationDataLake_name.dbfs.core.windows.net/parquetDataFolderName'\n",
    "\"\"\"\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b76342b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                        |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|Branch_ID                   |string                                                                           |NULL   |\n",
      "|Dealer_ID                   |string                                                                           |NULL   |\n",
      "|Model_ID                    |string                                                                           |NULL   |\n",
      "|Revenue                     |bigint                                                                           |NULL   |\n",
      "|Units_Sold                  |bigint                                                                           |NULL   |\n",
      "|Date_ID                     |string                                                                           |NULL   |\n",
      "|Day                         |int                                                                              |NULL   |\n",
      "|Month                       |int                                                                              |NULL   |\n",
      "|Year                        |int                                                                              |NULL   |\n",
      "|BranchName                  |string                                                                           |NULL   |\n",
      "|DealerName                  |string                                                                           |NULL   |\n",
      "|model_category              |string                                                                           |NULL   |\n",
      "|RevPerUnit                  |double                                                                           |NULL   |\n",
      "|                            |                                                                                 |       |\n",
      "|# Detailed Table Information|                                                                                 |       |\n",
      "|Name                        |spark_catalog.default.cardealers                                                 |       |\n",
      "|Type                        |EXTERNAL                                                                         |       |\n",
      "|Location                    |file:/C:/Users/Administrator/Documents/Code/Python/PySpark/MyDeltaLake/CarDealers|       |\n",
      "|Provider                    |delta                                                                            |       |\n",
      "|Owner                       |Administrator                                                                    |       |\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE EXTENDED CarDealers')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea15110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+--------------------------------+-----------+---------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|format|id                                  |name                            |description|location                                                                         |createdAt              |lastModified           |partitionColumns|clusteringColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |\n",
      "+------+------------------------------------+--------------------------------+-----------+---------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "|delta |83dbda18-8d8f-4eff-9b3a-edfb7968096a|spark_catalog.default.cardealers|NULL       |file:/C:/Users/Administrator/Documents/Code/Python/PySpark/MyDeltaLake/CarDealers|2025-07-29 13:47:57.279|2025-07-29 13:47:59.182|[]              |[]               |1       |61366      |{}        |1               |2               |[appendOnly, invariants]|\n",
      "+------+------------------------------------+--------------------------------+-----------+---------------------------------------------------------------------------------+-----------------------+-----------------------+----------------+-----------------+--------+-----------+----------+----------------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('DESCRIBE DETAIL CarDealers')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7050d56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+--------+----------+-------+---+-----+----+------------------------------------+------------------+--------------+----------+\n",
      "|Branch_ID|Dealer_ID|Model_ID|Revenue |Units_Sold|Date_ID|Day|Month|Year|BranchName                          |DealerName        |model_category|RevPerUnit|\n",
      "+---------+---------+--------+--------+----------+-------+---+-----+----+------------------------------------+------------------+--------------+----------+\n",
      "|BR2353   |DLR0196  |Mah-M176|7236359 |1         |DT01136|31 |12   |2020|Cadillac Automobile Company Building|Toyota Motors     |Mah           |7236359.0 |\n",
      "|BR2468   |DLR0107  |Cad-M37 |3108616 |1         |DT01185|30 |12   |2020|Kindel Building                     |Land Rover Motors |Cad           |3108616.0 |\n",
      "|BR2323   |DLR0193  |Mah-M173|6271301 |1         |DT01119|28 |12   |2020|Blankinship Motor Company Building  |Tazzari Motors    |Mah           |6271301.0 |\n",
      "|BR2448   |DLR0105  |Cad-M35 |4116309 |3         |DT01179|28 |12   |2020|Jennings Ford Automobile Dealership |Lamborghini Motors|Cad           |1372103.0 |\n",
      "|BR2313   |DLR0192  |Mah-M172|4347531 |1         |DT01113|27 |12   |2020|2008 NRHP-listed                    |Tatra Motors      |Mah           |4347531.0 |\n",
      "|BR2438   |DLR0104  |Cad-M34 |22956252|3         |DT01173|27 |12   |2020|Hupmobile Building                  |Lada Motors       |Cad           |7652084.0 |\n",
      "|BR2428   |DLR0103  |Cad-M33 |18247482|3         |DT01169|26 |12   |2020|Howard Motor Company Building       |KTM Motors        |Cad           |6082494.0 |\n",
      "|BR2303   |DLR0191  |Mah-M171|6380740 |2         |DT01109|26 |12   |2020|Atlantic Motor Company              |Tata Motors       |Mah           |3190370.0 |\n",
      "|BR2293   |DLR0190  |Mah-M170|9632678 |1         |DT01105|25 |12   |2020|Zion Automobils                     |Suzuki Motors     |Mah           |9632678.0 |\n",
      "|BR2283   |DLR0189  |Mah-M169|12791852|2         |DT01099|24 |12   |2020|Herald Motors                       |Sunbeam Motors    |Mah           |6395926.0 |\n",
      "+---------+---------+--------+--------+----------+-------+---+-----+----+------------------------------------+------------------+--------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('SELECT * FROM CarDealers ORDER BY Year DESC, Month DESC, Day DESC;')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(10, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66e166e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Table(name='CarDealers', catalog='spark_catalog', namespace=['default'], description=None, tableType='EXTERNAL', isTemporary=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark.catalog.getTable('CarDealers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f64ba8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1f42912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/C:/Users/Administrator/Documents/Code/Python/PySpark/spark-warehouse')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark.catalog.getDatabase('default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "965303b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:/C:/Users/Administrator/Documents/Code/Python/PySpark/spark-warehouse'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_spark.catalog.getDatabase('default').locationUri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab7be6",
   "metadata": {},
   "source": [
    "### HOW QUERY DIRECTLY FROM DELTA FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab24bc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------+--------+----------+-------+---+-----+----+-------------+------------------------------------+--------------+----------+\n",
      "|Branch_ID|Dealer_ID|Model_ID|Revenue |Units_Sold|Date_ID|Day|Month|Year|BranchName   |DealerName                          |model_category|RevPerUnit|\n",
      "+---------+---------+--------+--------+----------+-------+---+-----+----+-------------+------------------------------------+--------------+----------+\n",
      "|BR0074   |DLR0215  |Toy-M195|3163004 |2         |DT00045|19 |1    |2017|Anadol Motors|Wanderer Motors                     |Toy           |1581502.0 |\n",
      "|BR0075   |DLR0195  |Mah-M165|21878175|3         |DT00046|23 |9    |2017|Anadol Motors|Tornado Motors                      |Mah           |7292725.0 |\n",
      "|BR0077   |DLR0155  |Toy-M105|9867107 |1         |DT00046|31 |8    |2017|Anadol Motors|Piaggio Motors                      |Toy           |9867107.0 |\n",
      "|BR0078   |DLR0135  |Hyu-M75 |4583883 |3         |DT00047|5  |5    |2017|Anadol Motors|Morgan Motors                       |Hyu           |1527961.0 |\n",
      "|BR0080   |DLR0095  |Ram-M15 |14383990|2         |DT00048|11 |9    |2017|Anadol Motors|Isuzu Motors                        |Ram           |7191995.0 |\n",
      "|BR0081   |DLR0009  |Dod-M9  |24665256|3         |DT00048|9  |1    |2017|Ariel Motors |Ariel Motors                        |Dod           |8221752.0 |\n",
      "|BR0081   |DLR0076  |Nis-M263|2354637 |3         |DT00049|16 |5    |2017|Ariel Motors |Healey Motors                       |Nis           |784879.0  |\n",
      "|BR0082   |DLR0256  |Vol-M256|2563134 |3         |DT00049|13 |9    |2017|Ariel Motors |Ariel Motors                        |Vol           |854378.0  |\n",
      "|BR0083   |DLR0236  |For-M226|5756780 |1         |DT00050|18 |5    |2017|Ariel Motors |Cadillac Automobile Company Building|For           |5756780.0 |\n",
      "|BR0084   |DLR0216  |Toy-M196|4317585 |1         |DT00050|20 |1    |2017|Ariel Motors |Maybach  Motors                     |Toy           |4317585.0 |\n",
      "+---------+---------+--------+--------+----------+-------+---+-----+----+-------------+------------------------------------+--------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "_resultsAsSQL_DataFrame = _spark.sql('SELECT * FROM delta.`file:/C:/Users/Administrator/Documents/Code/Python/PySpark/MyDeltaLake/CarDealers`;')\n",
    "#\n",
    "_resultsAsSQL_DataFrame.show(10, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6630f47d",
   "metadata": {},
   "source": [
    "# Unity Metastore (ADLS)\n",
    "# |\n",
    "## Catalog\n",
    "# |\n",
    "### DataBase\n",
    "# |\n",
    "#### Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c98571",
   "metadata": {},
   "source": [
    "<!-- ![idempotency](https://thetightlycoupled.wordpress.com/wp-content/uploads/2020/02/idempotent.png \"idempotency\") -->\n",
    "<img src=\"./resources/idempotent.png\" alt=\"idempotency\" width=\"400\">\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"./resources/612ee17c-c597-4f53-bb3a-67eafb5f0e17_1080x994.png\" alt=\"idempotency\" height=\"500\">\n",
    "<!-- ![idempotency](./resources/612ee17c-c597-4f53-bb3a-67eafb5f0e17_1080x994.png \"idempotency\") -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea7c13d",
   "metadata": {},
   "source": [
    "##\n",
    "#### REF : https://medium.com/@riyukhandelwal/auto-loader-handling-incremental-etl-with-databricks-ae71687a281b\n",
    "##\n",
    "#### https://www.youtube.com/watch?v=njE89SwtPYI&t=02h33m33s\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636883ff",
   "metadata": {},
   "source": [
    "# Incrementally Loading Files:\n",
    "\n",
    "```python\n",
    "#\n",
    "## .option('cloudFiles.schemaEvolutionMode','addNewColumns') <<<<< this is the default, which means that if you don't included, will take it exactly the same\n",
    "#\n",
    "## ref : https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/schema\n",
    "## ref : https://medium.com/@nivethanvenkat28/spark-structured-streaming-for-incremental-batch-processing-using-databricks-autoloader-48316b36973b\n",
    "#\n",
    "_autoLoadedDataFrameFromFile = _spark.readStream \\\n",
    "\t.format('cloudFiles') \\\n",
    "\t.option('cloudFiles.format','csv') \\\n",
    "\t.option('cloudFiles.schemaLocation','file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Autoloaded/Checkpoint/') \\\n",
    "\t.option('cloudFiles.schemaEvolutionMode','addNewColumns') \\\n",
    "\t.option('schemaHints',True) \\\n",
    "\t.load('file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Autoloaded/Raw/')\n",
    "#\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb693aa1",
   "metadata": {},
   "source": [
    "Auto Loader supports the following modes for schema evolution, which you set in the option `cloudFiles.schemaEvolutionMode`:\n",
    "\n",
    "| Mode                     | Behavior on reading new column                                                                                                                                 |\n",
    "|--------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `addNewColumns` (default) | Stream fails. New columns are added to the schema. Existing columns do not evolve data types.                                                                 |\n",
    "| `rescue`                 | Schema is never evolved and <font color='red'>stream does not fail due to schema changes</font>. All new columns are recorded in the **rescued data column**.                    |\n",
    "| `failOnNewColumns`       | Stream fails. Stream does not restart unless the provided schema is updated, or the offending data file is removed.                                            |\n",
    "| `none`                   | Does not evolve the schema, new columns are ignored, and data is not rescued unless the `rescuedDataColumn` option is set. Stream does not fail due to schema changes. |\n",
    "\n",
    "REF : https://docs.databricks.com/aws/en/ingestion/cloud-object-storage/auto-loader/schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63de09",
   "metadata": {},
   "source": [
    "```python\n",
    "#\n",
    "_autoLoadedDataFrameFromFile.writeStream.format('delta) \\\n",
    "\t.option('checkpointLocation','file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Autoloaded/Checkpoint/') \\\n",
    "\t.start('file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Autoloaded/Data/')\n",
    "#\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d265d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9005e1d0",
   "metadata": {},
   "source": [
    "#### otro youtuber que tiene cosas interesantes de Databricks:\n",
    "#### REF : https://www.youtube.com/watch?v=-WyQh_ha3hI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5894ae94",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af14fe5e",
   "metadata": {},
   "source": [
    "# Volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d92536ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark_catalog\n",
      "Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='file:/C:/Users/Administrator/Documents/Code/Python/PySpark/spark-warehouse')\n",
      "file:/C:/Users/Administrator/Documents/Code/Python/PySpark/spark-warehouse\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "print(_spark.catalog.currentCatalog())\n",
    "print(_spark.catalog.getDatabase('default'))\n",
    "print(_spark.catalog.getDatabase('default').locationUri)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dfe3c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|current_schema()|\n",
      "+----------------+\n",
      "|         default|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.sql import functions\n",
    "#\n",
    "_spark.range(1).select(functions.current_schema()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632a9c4e",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "#\n",
    "## REF : https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-volumes\n",
    "#\n",
    "_query = \"CREATE EXTERNAL VOLUME IF NOT EXISTS spark_catalog.default.myExternalVolume COMMENT 'This is my example external volume' LOCATION 'file:///C:/Users/Administrator/Documents/Code/Python/PySpark/Volumes/'\" ## file:///C:/Users/.../Volumes/parquet/\n",
    "#\n",
    "_spark.sql(sqlQuery=_query)\n",
    "#\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c991baa5",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "#\n",
    "## REF : https://docs.databricks.com/aws/en/sql/language-manual/sql-ref-volumes\n",
    "#\n",
    "_query = \"SELECT * FROM parquet.'/Volumes/spark_catalog/default/myExternalVolume/parquet'\n",
    "#\n",
    "_spark.sql(sqlQuery=_query)\n",
    "#\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128030e3",
   "metadata": {},
   "source": [
    "## Data Versioning & Time Travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c20af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "## _query = 'DROP TABLE spark_catalog.default.myTimeTravelTable'\n",
    "## _spark.sql(sqlQuery=_query)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "595846d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"CREATE TABLE IF NOT EXISTS spark_catalog.default.myTimeTravelTable (id INT, name STRING, salary FLOAT) USING DELTA LOCATION 'file:///C:/Users/Administrator/Documents/Code/Python/PySpark/TimeTravelTable'\"\n",
    "_spark.sql(sqlQuery=_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cff4fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"\"\"INSERT INTO spark_catalog.default.myTimeTravelTable VALUES (0,'a',0.1),(1,'b',0.2),(2,'c',0.3)\"\"\"\n",
    "### print(_query)\n",
    "_spark.sql(sqlQuery=_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "515140a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"DELETE FROM spark_catalog.default.myTimeTravelTable WHERE id = 0\"\n",
    "### print(_query)\n",
    "_spark.sql(sqlQuery=_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b71a818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation   |operationParameters                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                            |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-07-30 12:46:43.993|NULL  |NULL    |CREATE TABLE|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}|NULL|NULL    |NULL     |NULL       |Serializable  |true         |{}                                                                                                                                                                                                                                                                                                                          |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|1      |2025-07-30 12:46:52.776|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                                            |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 966}                                                                                                                                                                                                                                                                  |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|2      |2025-07-30 12:47:00.738|NULL  |NULL    |DELETE      |{predicate -> [\"(id#443 = 0)\"]}                                                                |NULL|NULL    |NULL     |1          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 966, numCopiedRows -> 2, numDeletionVectorsAdded -> 0, executionTimeMs -> 2427, numDeletionVectorsUpdated -> 0, numAddedFiles -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, numDeletedRows -> 1, scanTimeMs -> 2233, numAddedBytes -> 953, rewriteTimeMs -> 192}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.sql.functions import col\n",
    "#\n",
    "_query = \"DESCRIBE HISTORY spark_catalog.default.myTimeTravelTable\"\n",
    "### print(_query)\n",
    "_dfHistory = _spark.sql(sqlQuery=_query).sort(col('version').asc())\n",
    "_dfHistory.show(500, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02e5a972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_query = \"RESTORE TABLE spark_catalog.default.myTimeTravelTable TO VERSION AS OF 1\"\n",
    "### print(_query)\n",
    "_spark.sql(sqlQuery=_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96b6661d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|id |name|salary|\n",
      "+---+----+------+\n",
      "|0  |a   |0.1   |\n",
      "|1  |b   |0.2   |\n",
      "|2  |c   |0.3   |\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_query = \"SELECT * FROM spark_catalog.default.myTimeTravelTable ORDER BY id ASC\"\n",
    "### print(_query)\n",
    "_spark.sql(sqlQuery=_query).show(500, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9995596f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation   |operationParameters                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                            |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|0      |2025-07-30 12:46:43.993|NULL  |NULL    |CREATE TABLE|{partitionBy -> [], clusterBy -> [], description -> NULL, isManaged -> false, properties -> {}}|NULL|NULL    |NULL     |NULL       |Serializable  |true         |{}                                                                                                                                                                                                                                                                                                                          |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|1      |2025-07-30 12:46:52.776|NULL  |NULL    |WRITE       |{mode -> Append, partitionBy -> []}                                                            |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 966}                                                                                                                                                                                                                                                                  |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|2      |2025-07-30 12:47:00.738|NULL  |NULL    |DELETE      |{predicate -> [\"(id#443 = 0)\"]}                                                                |NULL|NULL    |NULL     |1          |Serializable  |false        |{numRemovedFiles -> 1, numRemovedBytes -> 966, numCopiedRows -> 2, numDeletionVectorsAdded -> 0, executionTimeMs -> 2427, numDeletionVectorsUpdated -> 0, numAddedFiles -> 1, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, numDeletedRows -> 1, scanTimeMs -> 2233, numAddedBytes -> 953, rewriteTimeMs -> 192}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|3      |2025-07-30 12:51:37.377|NULL  |NULL    |RESTORE     |{version -> 1, timestamp -> NULL}                                                              |NULL|NULL    |NULL     |2          |Serializable  |false        |{numRestoredFiles -> 1, removedFilesSize -> 953, numRemovedFiles -> 1, restoredFilesSize -> 966, numOfFilesAfterRestore -> 1, tableSizeAfterRestore -> 966}                                                                                                                                                                 |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|4      |2025-07-30 12:52:46.216|NULL  |NULL    |RESTORE     |{version -> 1, timestamp -> NULL}                                                              |NULL|NULL    |NULL     |3          |Serializable  |false        |{numRestoredFiles -> 0, removedFilesSize -> 0, numRemovedFiles -> 0, restoredFilesSize -> 0, numOfFilesAfterRestore -> 1, tableSizeAfterRestore -> 966}                                                                                                                                                                     |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+------------+-----------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from pyspark.sql.functions import col\n",
    "#\n",
    "_query = \"DESCRIBE HISTORY spark_catalog.default.myTimeTravelTable\"\n",
    "### print(_query)\n",
    "_dfHistory = _spark.sql(sqlQuery=_query).sort(col('version').asc())\n",
    "_dfHistory.show(500, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c2da99",
   "metadata": {},
   "source": [
    "## OPTIMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5729d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|id |name|salary|\n",
      "+---+----+------+\n",
      "|0  |a   |0.1   |\n",
      "|1  |b   |0.2   |\n",
      "|2  |c   |0.3   |\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "_spark.sql(sqlQuery='OPTIMIZE spark_catalog.default.myTimeTravelTable')\n",
    "#\n",
    "#\n",
    "_query = \"SELECT * FROM spark_catalog.default.myTimeTravelTable ORDER BY id ASC\"\n",
    "### print(_query)\n",
    "_spark.sql(sqlQuery=_query).show(500, truncate=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2c6e7e",
   "metadata": {},
   "source": [
    "## 🚀 SQL `OPTIMIZE` Command in PySpark\n",
    "\n",
    "The `OPTIMIZE` command in PySpark SQL is designed to **enhance performance** and **reduce the overhead** of querying Delta Lake tables by compacting small files.\n",
    "\n",
    "### 🔧 Purpose\n",
    "- Merges small data files into larger ones\n",
    "- Boosts read efficiency and lowers latency\n",
    "- Ideal for large-scale analytic workloads\n",
    "\n",
    "### 📌 Syntax\n",
    "```sql\n",
    "OPTIMIZE table_name [WHERE predicate]\n",
    "```\n",
    "- `table_name`: The Delta Lake table you want to optimize\n",
    "- `WHERE predicate`: (Optional) Filter rows to optimize specific partitions or data ranges\n",
    "\n",
    "🧠 Key Features\n",
    "- Targets **Delta Lake** tables only\n",
    "- Can be used with **Z-Ordering** to optimize column-specific data layouts\n",
    "\n",
    "## 🧲 Example\n",
    "```sql\n",
    "-- Optimizes entire table\n",
    "OPTIMIZE sales_data;\n",
    "\n",
    "-- Optimizes specific partition\n",
    "OPTIMIZE sales_data WHERE region = 'West';\n",
    "```\n",
    "\n",
    "### 📌 Usage\n",
    "- Applies to Delta Lake tables only\n",
    "- Optional filtering with `WHERE` clause to optimize specific partitions or subsets of data\n",
    "\n",
    "### 🧠 Features\n",
    "- Supports partition-level optimization\n",
    "- Can be paired with **Z-Ordering** to cluster data by specific columns for improved scan performance\n",
    "\n",
    "### ⚠️ Notes\n",
    "- Only usable on tables stored in Delta format\n",
    "- Most effective in environments with frequent data insertions or streaming workloads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d69364",
   "metadata": {},
   "source": [
    "## ZORDERING (liquid clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dc80522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------+\n",
      "|id |name|salary|\n",
      "+---+----+------+\n",
      "|0  |a   |0.1   |\n",
      "|1  |b   |0.2   |\n",
      "|2  |c   |0.3   |\n",
      "+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "## REF : https://learn.microsoft.com/en-us/azure/databricks/delta/clustering\n",
    "#\n",
    "## REF : https://delta.io/blog/liquid-clustering/\n",
    "#\n",
    "_spark.sql(sqlQuery='OPTIMIZE spark_catalog.default.myTimeTravelTable ZORDER BY id')\n",
    "#\n",
    "#\n",
    "_query = \"SELECT * FROM spark_catalog.default.myTimeTravelTable ORDER BY id ASC\"\n",
    "### print(_query)\n",
    "_spark.sql(sqlQuery=_query).show(500, truncate=False)\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
